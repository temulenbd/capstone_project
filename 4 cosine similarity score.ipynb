{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IV: CALCULATION OF COSINE SIMILARITY\n",
    "\n",
    "This section focuses entirely on calculating the cosine similarity between the participant data and the job ads data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GENERAL\n",
    "\n",
    "- **load module**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load nessesary libraries.\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "import torch\n",
    "import psutil\n",
    "import gpustat\n",
    "import warnings\n",
    "import platform\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "from scipy.sparse import hstack\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import KeyedVectors\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics.pairwise import cosine_similarity as cos\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import BertForSequenceClassification\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **check computational environment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the software and hardware configurations used for conducting the experiment.\n",
    "print('WINDOWS VERSION:', platform.platform())\n",
    "print('PYTHON VERSION:', sys.version)\n",
    "print('CPU CORE:', psutil.cpu_count(logical=False))\n",
    "print('CPU SPEED:', psutil.cpu_freq())\n",
    "print('GPU:', gpustat.new_query().gpus[0].name)\n",
    "print(f'RAM: {psutil.virtual_memory().total/(1024 ** 3):.2f} GB')\n",
    "print(f\"HARD DRIVE: {psutil.disk_usage('/').total/(1024 ** 3):.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **load dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*job seekers*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the experiment participants dataset.\n",
    "df_jobseeker = pd.read_csv('data_jobseeker.csv', index_col=None)\n",
    "print(\"The shape of the joob seekers' data frame is:\", df_jobseeker.shape)\n",
    "\n",
    "df_jobseeker.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first dataset consists of 3 rows and 8 columns of data collected from experiment participants through interviews. The last three columns in this DataFrame (DF), which contain text data on education, skill, and experience, are intended to be used for analysis. Calculating the cosine score for each column individually is impractical and illogical. Therefore, it is necessary to combine these columns into a single one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply minor modifications for further use.\n",
    "df_jobseeker['combined_info'] = df_jobseeker.education + '. ' + df_jobseeker.skill + '. ' + df_jobseeker.experience + '.'\n",
    "df_jobseeker.drop(['education', 'skill', 'experience'], axis=1, inplace=True)\n",
    "\n",
    "df_jobseeker.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having merged the text data into a single column, it is essential to perform a word count. This step will guide us in determining the appropriate approach for processing this text in the subsequent analytical stages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the word count for each ad and add its values to a new column.\n",
    "df_jobseeker['word_count'] = df_jobseeker['combined_info'].apply(lambda x: len(x.split()))\n",
    "\n",
    "df_jobseeker.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*job ads*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the online job ads dataset and apply minor modifications for further use.\n",
    "df_jobads = pd.read_csv('data_jobads_final.csv', index_col=None)\n",
    "df_jobads['job_description'] = df_jobads['job_description'].str.replace('\\n', ' ')\n",
    "df_jobads = df_jobads.dropna().reset_index(drop=True)\n",
    "\n",
    "print(\"The shape of the joob ads' data frame is:\", df_jobads.shape)\n",
    "df_jobads.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second dataset consists of 1166 rows and 6 columns of data scraped from Indeed.com. The most essential column in this DF is the one with job descriptions. Similarly to the first DF, counting the words for each row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobads['word_count'] = df_jobads['job_description'].apply(lambda x: len(x.split()))\n",
    "df_jobads.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All necessary libraries have been imported, and the datasets are also laoded and ready for use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. WITH FINE-TUNED BERT MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this sub-section, the text columns from both DFs are fed into Bert's fine-tuned encoding layers, and the resulting text representations from the last hidden layer are collected for cosine similarity computation. For demonstration purposes lets run the test for only one row value and retrieve the final hidden state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigning the text for demonstration to a variable.\n",
    "input_text_test = df_jobseeker.iat[0, -2]\n",
    "\n",
    "# Initialize a fine-tuned model with the hidden state output enabled.\n",
    "model = BertForSequenceClassification.from_pretrained('ft_bert_temuulen2', output_hidden_states=True)\n",
    "\n",
    "# Initialize a tokenizer used for the fine-tuned model.\n",
    "tokenizer = AutoTokenizer.from_pretrained('ft_bert_temuulen_tokenizer2')\n",
    "\n",
    "# Tokenize the input text and convert it to PyTorch tensors.\n",
    "inputs = tokenizer(input_text_test, return_tensors='pt')\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous cell, the test text specified for demonstration purposes was assigned to a variable and tokenized. The results were formatted as tensors to be compatible with our deep learning framework, PyTorch in this instance. The output of the cell shows that input itself consists of **input_ids** and **attention_mask** values, which are important for further procesing, as well as **token_type_ids** values, which are optional for the current context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a forward pass through the model to get the hidden states.\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# Extract the last hidden states from the model outputs.\n",
    "last_hidden_states = outputs.hidden_states[-1]\n",
    "\n",
    "print('The size of the last hidden state tensor is:', last_hidden_states.shape, '\\n')\n",
    "print('The data type of the last hidden state tensor is:', type(last_hidden_states), '\\n')\n",
    "print(last_hidden_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following tokenization, the input values were passed forward through the model, resulting in the extraction of a torch tensor representing hidden states with dimensions of ([1, 44, 768]). This tensor will then be used for cosine similarity calculations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Experiment\n",
    "\n",
    "The test demonstration went well and the tensor was successfully extracted. Now lets begin the experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting the timer to track the execution duration.\n",
    "start = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*initialize the model*\n",
    "\n",
    "The encoding model has been fine-tuned using the **bert-based-uncased** architecture for text sequence classification and was imported from the personal drive. The tokenizer employed is HuggingFace's autotokenizer, which automatically selects and pairs with the most suitable tokenizer for the model. In this instance, it is the **BertTokenizer**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a fine-tuned model with the hidden state output enabled.\n",
    "model = BertForSequenceClassification.from_pretrained('ft_bert_temuulen2', output_hidden_states=True)\n",
    "\n",
    "# Initialize a tokenizer used for the fine-tuned model.\n",
    "tokenizer = AutoTokenizer.from_pretrained('ft_bert_temuulen_tokenizer2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*load the dataset*\n",
    "\n",
    "The dataset used in this implementation is a duplicate of the primary DFs containing information about job seekers and job advertisements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bert_js = df_jobseeker.copy()\n",
    "df_bert_ja = df_jobads.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*initialize the gpu* (optional)\n",
    "\n",
    "To enhance the effectiveness of managing matrix and tensor operations, the CUDA device was created. This capability represents a key advantage of utilizing the BERT model within the Torch framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check whether CUDA is accessible and, if so, create a CUDA device.\n",
    "cuda_available = torch.cuda.is_available()\n",
    "cuda_device= torch.cuda.get_device_name(0)\n",
    "\n",
    "if cuda_available == True:\n",
    "    device = torch.device('cuda')\n",
    "    print('CUDA was successfully installed and compiled on my device.')\n",
    "    print('CUDA device name is:', cuda_device)\n",
    "else:\n",
    "    print('Cuda in not available')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting the encoding process, it's essential to check the word count to ensure that it doesn't surpass 510, due to a constraint associated with the BERT model. If the word count exceed this threshold, it is necessary to formulate a new strategy for obtaining the encoded value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The total number of rows having word counts greater than 510 in the first DF is:', df_bert_js[df_bert_js['word_count'] > 510].shape[0])\n",
    "print('The total number of rows having word counts greater than 510 in the second DF is:', df_bert_ja[df_bert_ja['word_count'] > 510].shape[0])\n",
    "print('The word count for the longest text is:', df_bert_ja.iat[df_bert_ja['word_count'].idxmax(), -1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*create custom function*\n",
    "\n",
    "From the output observed in the preceding cell, it is clear that the DF for job seekers does not contain entries exceeding the 510-word limit, allowing the definition of a standard custom function for tokenization and extraction of the last hidden state without additional conditions. Conversely, the DF for job advertisements contains 236 entries surpassing the 510-word threshold, with the longest text totaling 3145 words. To process these inputs through the model, a custom function incorporating special conditions must be developed and applied. The upcoming two custom functions are designed specifically for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom function to extract the final layer encodings from BERT, without conditions.\n",
    "def process_text(text):\n",
    "    \n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer(text, return_tensors='pt')\n",
    "    \n",
    "    # Pass the tokenized input through the model.\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Retrieve the last hidden states from the model's outputs.\n",
    "    last_hidden_states = outputs.hidden_states[-1]\n",
    "    \n",
    "    return last_hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom function to extract the final layer encodings from BERT, with conditions.\n",
    "def embed_with_bert(df_column):\n",
    "    \n",
    "    embedded_texts = []\n",
    "    \n",
    "    # Iterate through each text in the DataFrame column.\n",
    "    for text in df_column:\n",
    "        \n",
    "        # Tokenize each text without adding special tokens and without truncation or padding.\n",
    "        tokens = tokenizer(text, add_special_tokens=False, return_tensors='pt', truncation=False, padding=False)['input_ids'].squeeze()\n",
    "        token_length = len(tokens)\n",
    "        \n",
    "        # If the token length is less than or equal to 512, process it normally.\n",
    "        if token_length <= 512:\n",
    "            inputs = tokenizer(text, return_tensors='pt').to(device)\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs)\n",
    "            last_hidden_states = outputs.hidden_states[-1].cpu()  \n",
    "            embedded_texts.append(last_hidden_states)\n",
    "            \n",
    "        # If the token length is greater than 512, split it into sliding windows withot lapping.\n",
    "        else:\n",
    "            max_length = 512\n",
    "            stride = 0\n",
    "            tokens = tokenizer(text, add_special_tokens=False, return_tensors='pt', truncation=False, padding=False)['input_ids'].squeeze().to(device)\n",
    "            token_windows = [tokens[i:i+max_length] for i in range(0, len(tokens), max_length - stride)]\n",
    "            \n",
    "            all_hidden_states = []\n",
    "            \n",
    "            # Add special tokens (CLS and SEP) and truncate if needed.\n",
    "            for window in token_windows:\n",
    "                window = torch.cat([torch.tensor([tokenizer.cls_token_id], device=device), window, torch.tensor([tokenizer.sep_token_id], device=device)])\n",
    "                if len(window) > max_length:\n",
    "                    window = torch.cat((window[:max_length-1], torch.tensor([tokenizer.sep_token_id], device=device)))\n",
    "                inputs = {'input_ids': window.unsqueeze(0)}\n",
    "                with torch.no_grad():\n",
    "                    outputs = model(**inputs)\n",
    "                hidden_states = outputs.hidden_states[-1].cpu()  \n",
    "                all_hidden_states.append(hidden_states)\n",
    "            \n",
    "            # Concatenate all hidden states from each sliding window.\n",
    "            embedded_texts.append(torch.cat(all_hidden_states, dim=1))\n",
    "            \n",
    "    return embedded_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Furthermore, as observed in the previous test demonstration, the text that passes through the encoders generates a hidden states tensor with three dimensions. To keep the textual information without aggregating these dimensions, it is necessary to define a custom function. The function below processes the tensor of a user's text, computes the cosine score for each pair, and then returns the average score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a costum function that generates the evarage cosine similarity between the user's tensor and a job ad's tensor.\n",
    "def calculate_average_similarity(tensor_user, tensor_ad):\n",
    "    \n",
    "    # Squeeze dimensions if the tensors have a batch dimension.\n",
    "    tensor_user = tensor_user.squeeze(0) if tensor_user.dim() == 3 else tensor_user\n",
    "    tensor_ad = tensor_ad.squeeze(0) if tensor_ad.dim() == 3 else tensor_ad\n",
    "\n",
    "    tensor_ad = tensor_ad.to(tensor_user.device)\n",
    "\n",
    "    # Initialize a similarity matrix with zeros.\n",
    "    similarity_matrix = torch.zeros(tensor_user.size(0), tensor_ad.size(0), device=tensor_user.device)\n",
    "    \n",
    "    # Calculate cosine similarity for each pair of vectors.\n",
    "    for i in range(tensor_user.size(0)):\n",
    "        for j in range(tensor_ad.size(0)):\n",
    "            similarity_matrix[i, j] = F.cosine_similarity(tensor_user[i].unsqueeze(0), tensor_ad[j].unsqueeze(0), dim=1)\n",
    "            \n",
    "    # Calculate the average similarity and convert it to a Python float.\n",
    "    average_similarity = torch.mean(similarity_matrix).item()\n",
    "    \n",
    "    return average_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*encode the text*\n",
    "\n",
    "Using the custom functions created earlier to process each DF and extract the tensor of the final hidden state layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply function and create a new column with the extracted results.\n",
    "df_bert_js['last_layer'] = df_bert_js.iloc[:, -2].apply(process_text)\n",
    "\n",
    "print('The shape of the first tensor:', df_bert_js.iat[0, -1].shape, '\\n')\n",
    "print('The shape of the second tensor:', df_bert_js.iat[1, -1].shape, '\\n')\n",
    "print('The shape of the third tensor:', df_bert_js.iat[2, -1].shape, '\\n')\n",
    "print(df_bert_js.iat[0, -1], '\\n')\n",
    "\n",
    "# Check the Data Frame.\n",
    "df_bert_js.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move the model to the GPU.\n",
    "model.to(device)\n",
    "\n",
    "# Apply the 'embed_with_bert' function to each ad.\n",
    "df_bert_ja['tensors'] = df_bert_ja['job_description'].apply(lambda x: embed_with_bert([x])[0])\n",
    "\n",
    "# Check the random cell to see the results.\n",
    "print(df_bert_ja.iat[0, -1].shape, '\\n')\n",
    "print(df_bert_ja.iat[0, -1], '\\n')\n",
    "\n",
    "# Check the Data Frame.\n",
    "df_bert_ja.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results from the previous cells indicate that the tensors generated by processing each text entry from the 'combined_info' column through the encoding layers of the fine-tuned models maintain consistent dimensions in the first and third positions. This consistency is due to the fact that each encoder handles a single sample at a time, with a batch size of one, and represents each token in the text with a 768-feature vector. However, the number of tokens in the second dimensions, representing each text, varies and slightly exceeds the actual word count of each text. This variability is because of the WordPiece tokenization approach used by the BERT model, which breaks down words into smaller pieces if they are not present in the tokenizer's lexicon. This approach enables the model to more effectively manage unrecognized words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*calculate cosine*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The cosine similarity between the texts from user1 and user2 is:', calculate_average_similarity(df_bert_js.iat[0, -1], df_bert_js.iat[1, -1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the user1's tensor and move it to the GPU.\n",
    "user1_tensor = df_bert_js.iat[0, -1]\n",
    "user1_tensor = user1_tensor.to(device)\n",
    "\n",
    "# Get the user2's tensor and move it to the GPU.\n",
    "user2_tensor = df_bert_js.iat[1, -1]\n",
    "user2_tensor = user2_tensor.to(device)\n",
    "\n",
    "# Get the user3's tensor and move it to the GPU.\n",
    "user3_tensor = df_bert_js.iat[2, -1]\n",
    "user3_tensor = user3_tensor.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the calculation of average cosine similarity function to each job ad's tensor.\n",
    "df_bert_ja['cosine_user1'] = df_bert_ja.iloc[:, -1].apply(lambda x: calculate_average_similarity(user1_tensor, x.to(device)))\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the calculation of average cosine similarity function to each job ad's tensor.\n",
    "df_bert_ja['cosine_user2'] = df_bert_ja.iloc[:, -2].apply(lambda x: calculate_average_similarity(user2_tensor, x.to(device)))\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the calculation of average cosine similarity function to each job ad's tensor.\n",
    "df_bert_ja['cosine_user3'] = df_bert_ja.iloc[:, -3].apply(lambda x: calculate_average_similarity(user3_tensor, x.to(device)))\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the encoded column from the Data Frame (it takes up too much memory and is no longer needed).\n",
    "df_bert_ja = df_bert_ja.drop(columns=['tensors']) \n",
    "\n",
    "df_bert_ja.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bert_ja.to_csv('cosine-bert.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end = time.time()\n",
    "\n",
    "print(f'The calculation of cosine similarity score using fine-tuned Bert model was completed in: {int((end - start)) // 60} minutes and {int((end - start)) % 60} seconds.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. WITH PRE-TRAINED WORD2VEC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting the timer to track the execution duration.\n",
    "start = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*initialize the model*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained Word2Vec model\n",
    "word2vec = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*load the dataset*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word2vec_js = df_jobseeker.copy()\n",
    "df_word2vec_ja = df_jobads.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*preprocessing*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lowercasing and tokenizing\n",
    "def preprocess_text_word2vec(text):\n",
    "    # Lowercasing\n",
    "    text = text.lower()\n",
    "    # Removing punctuation\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word2vec_js['processed_ci'] = df_word2vec_js['combined_info'].apply(preprocess_text_word2vec)\n",
    "df_word2vec_ja['processed_jd'] = df_word2vec_ja['job_description'].apply(preprocess_text_word2vec)\n",
    "\n",
    "df_word2vec_js.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " For each entry in text column, the custom function tokenizes the text into words. Then it filters out the words not in the Word2Vec vocabulary, and then generate embeddings for each word. A common approach is to average these word vectors to get a single vector that represents the entire text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*embedding*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_tokens(tokens_list, model):\n",
    "    vectors = [model[word] for word in tokens_list if word in model]\n",
    "    if vectors:\n",
    "        # Averaging the vectors (You could choose another aggregation method)\n",
    "        embedding = np.mean(vectors, axis=0)\n",
    "    else:\n",
    "        # Use a zero vector if none of the tokens were found in the Word2Vec model\n",
    "        embedding = np.zeros(model.vector_size)\n",
    "        \n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to embed each row's tokens in the DataFrame\n",
    "df_word2vec_js['vectors'] = df_word2vec_js['processed_ci'].apply(lambda x: embed_tokens(x, word2vec))\n",
    "df_word2vec_ja['vectors'] = df_word2vec_ja['processed_jd'].apply(lambda x: embed_tokens(x, word2vec))\n",
    "# This will add a new column 'word2vec_embedding' where each row contains the aggregated Word2Vec embedding for its tokens\n",
    "\n",
    "print('The shape of the first tensor:', df_word2vec_js.iat[0, -1].shape, '\\n')\n",
    "print('The shape of the second tensor:', df_word2vec_js.iat[1, -1].shape, '\\n')\n",
    "print(df_word2vec_js.iat[0, -1], '\\n')\n",
    "\n",
    "df_word2vec_ja.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to calculate cosine similarity (dot product in this case)\n",
    "def cos(vector1, vector2):\n",
    "    return np.dot(vector1, vector2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user1_vector = df_word2vec_js.iat[0, -1].copy()\n",
    "user2_vector = df_word2vec_js.iat[1, -1].copy()\n",
    "user3_vector = df_word2vec_js.iat[2, -1].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the cosine similarity for each row\n",
    "df_word2vec_ja['cos_user1'] = df_word2vec_ja['vectors'].apply(lambda x: cos(x, user1_vector))\n",
    "\n",
    "df_word2vec_ja['cos_user2'] = df_word2vec_ja['vectors'].apply(lambda x: cos(x, user2_vector))\n",
    "\n",
    "df_word2vec_ja['cos_user3'] = df_word2vec_ja['vectors'].apply(lambda x: cos(x, user3_vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word2vec_ja.drop(columns=['processed_jd', 'vectors'], inplace=True)\n",
    "\n",
    "df_word2vec_ja.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word2vec_ja.to_csv('cosine-word2vec.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end = time.time()\n",
    "\n",
    "print(f'The calculation was completed in: {int((end - start)) // 60} minutes and {int((end - start)) % 60} seconds.')\n",
    "\n",
    "print(f'The calculation of cosine similarity using pretrained word2vec model was completed in: {int((end - start)) // 60} minutes and {int((end - start)) % 60} seconds.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. WITH TF-IDF AND BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting the timer to track the execution duration.\n",
    "start = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*initialize the tools*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "bow_vectorizer = CountVectorizer()\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*load the dataset*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tfidf_js = df_jobseeker.copy()\n",
    "df_tfidf_ja = df_jobads.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*preprocessing*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text_tfidf(text):\n",
    "    # Lowercasing\n",
    "    text = text.lower()\n",
    "    # Removing punctuation\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "    # Removing stopwords and lemmatization\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    processed_tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "    # Re-joining tokens\n",
    "    processed_text = ' '.join(processed_tokens)\n",
    "    \n",
    "    return processed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tfidf_js['processed_ci'] = df_tfidf_js['combined_info'].apply(preprocess_text_tfidf)\n",
    "df_tfidf_ja['processed_jd'] = df_tfidf_ja['job_description'].apply(preprocess_text_tfidf)\n",
    "df_tfidf_js.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_rows = pd.DataFrame([[''] * len(df_tfidf_ja.columns)] * 3, columns=df_tfidf_ja.columns)\n",
    "df_tfidf_ja = pd.concat([empty_rows, df_tfidf_ja], ignore_index=True)\n",
    "\n",
    "df_tfidf_ja.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values_to_add = df_tfidf_js['processed_ci'].tolist()[:3]\n",
    "df_tfidf_ja['processed_jd'].iloc[:3] = values_to_add\n",
    "\n",
    "df_tfidf_ja.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df_tfidf_ja['processed_jd'])\n",
    "bow_matrix = bow_vectorizer.fit_transform(df_tfidf_ja['processed_jd'])\n",
    "combined_matrix = hstack([tfidf_matrix, bow_matrix])\n",
    "\n",
    "# Convert each row of the TF-IDF matrix to a list and store in a new DataFrame column\n",
    "df_tfidf_ja['vectors'] = list(combined_matrix.toarray())\n",
    "df_tfidf_ja.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_df = df_tfidf_ja.iat[0, -1]\n",
    "print(check_df.shape)\n",
    "print(type(check_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors_array = pd.DataFrame(df_tfidf_ja['vectors'].tolist())\n",
    "vectors_array.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_vectors = normalize(vectors_array, norm='l2', axis=1)\n",
    "normalized_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tfidf_ja['normolized_vec'] = normalized_vectors.tolist()\n",
    "df_tfidf_ja.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Cosine calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors_tf = np.array(df_tfidf_ja['normolized_vec'].tolist()).copy()\n",
    "\n",
    "user1_vector_tf = vectors_tf[0].reshape(1, -1).copy()\n",
    "user2_vector_tf = vectors_tf[1].reshape(1, -1).copy()\n",
    "user3_vector_tf = vectors_tf[2].reshape(1, -1).copy()\n",
    "\n",
    "print(vectors_tf.shape)\n",
    "print(user1_vector_tf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarities = cos(user1_vector_tf, vectors_tf).flatten()\n",
    "df_tfidf_ja['cos_user1'] = cosine_similarities\n",
    "\n",
    "cosine_similarities = cos(user2_vector_tf, vectors_tf).flatten()\n",
    "df_tfidf_ja['cos_user2'] = cosine_similarities\n",
    "\n",
    "cosine_similarities = cos(user3_vector_tf, vectors_tf).flatten()\n",
    "df_tfidf_ja['cos_user3'] = cosine_similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slicing the DataFrame to exclude the first three rows\n",
    "df_tfidf_ja = df_tfidf_ja.iloc[3:].reset_index(drop=True)\n",
    "\n",
    "df_tfidf_ja.drop(columns=['processed_jd', 'vectors', 'normolized_vec'], inplace=True)\n",
    "\n",
    "df_tfidf_ja.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tfidf_ja.to_csv('cosine-tfidf.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end = time.time()\n",
    "\n",
    "print(f'The calculation of cosine similarity using TF-IDF and BoW was completed in: {int((end - start)) // 60} minutes and {int((end - start)) % 60} seconds.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
