{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART III: CLASSIFICATION (pre-trained word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "from gensim.models import Word2Vec\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import plotly.figure_factory as ff\n",
    "from joblib import dump\n",
    "from joblib import load\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA was successfully installed and compiled on my device.\n",
      "CUDA device name is: NVIDIA GeForce GTX 1650\n"
     ]
    }
   ],
   "source": [
    "# Download a compatible Pytorch version.\n",
    "# !pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "\n",
    "# Check whether CUDA is accessible.\n",
    "cuda_available = torch.cuda.is_available()\n",
    "cuda_device= torch.cuda.get_device_name(0)\n",
    "\n",
    "if cuda_available == True:\n",
    "    print('CUDA was successfully installed and compiled on my device.')\n",
    "    print('CUDA device name is:', cuda_device)\n",
    "else:\n",
    "    print('Cuda in not available')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pretrained Word2Vec model\n",
    "pretrained_model = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\temulenbd\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\temulenbd\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Tokenize text\n",
    "    tokens = word_tokenize(text)\n",
    "    # Convert to lower case\n",
    "    tokens = [word.lower() for word in tokens]\n",
    "    # Remove punctuation and non-alphabetic tokens\n",
    "    words = [word for word in tokens if word.isalpha()]\n",
    "    # Remove stop words\n",
    "    words = [word for word in words if not word in stop_words]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_description</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>silver stream healthcare group offer great emp...</td>\n",
       "      <td>registered_nurse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>create a better future for yourself  recruitne...</td>\n",
       "      <td>registered_nurse</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     job_description             label\n",
       "0  silver stream healthcare group offer great emp...  registered_nurse\n",
       "1  create a better future for yourself  recruitne...  registered_nurse"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import finalized dataset as pandas data frame.\n",
    "df = pd.read_csv('data_jobads_final.csv', index_col=None)\n",
    "\n",
    "# Apply the final touch.\n",
    "df['job_description'] = df['job_description'].str.replace('\\n', ' ')\n",
    "df = df.dropna()\n",
    "df = df.iloc[:,-2:]\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_description</th>\n",
       "      <th>label</th>\n",
       "      <th>processed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>silver stream healthcare group offer great emp...</td>\n",
       "      <td>registered_nurse</td>\n",
       "      <td>[silver, stream, healthcare, group, offer, gre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>create a better future for yourself  recruitne...</td>\n",
       "      <td>registered_nurse</td>\n",
       "      <td>[create, better, future, recruitnet, internati...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     job_description             label  \\\n",
       "0  silver stream healthcare group offer great emp...  registered_nurse   \n",
       "1  create a better future for yourself  recruitne...  registered_nurse   \n",
       "\n",
       "                                      processed_text  \n",
       "0  [silver, stream, healthcare, group, offer, gre...  \n",
       "1  [create, better, future, recruitnet, internati...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['processed_text'] = df['job_description'].apply(preprocess_text)\n",
    "\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'registered_nurse', 1: 'electrician', 2: 'data_analyst'}\n",
      "{'registered_nurse': 0, 'electrician': 1, 'data_analyst': 2}\n"
     ]
    }
   ],
   "source": [
    "# Create 'id2label', 'label2id' variables for mapping the labels.\n",
    "labels = df['label'].unique().tolist()\n",
    "\n",
    "num_labels = len(labels)\n",
    "id2label = {id:label for id, label in enumerate(labels)}\n",
    "label2id = {label: id for id, label in enumerate(labels)}\n",
    "\n",
    "print(id2label)\n",
    "print(label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_description</th>\n",
       "      <th>label</th>\n",
       "      <th>processed_text</th>\n",
       "      <th>label_encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1164</th>\n",
       "      <td>the role our operations analysts are responsib...</td>\n",
       "      <td>data_analyst</td>\n",
       "      <td>[role, operations, analysts, responsible, mana...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1165</th>\n",
       "      <td>insurance analyst permanent dublin negotiable ...</td>\n",
       "      <td>data_analyst</td>\n",
       "      <td>[insurance, analyst, permanent, dublin, negoti...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        job_description         label  \\\n",
       "1164  the role our operations analysts are responsib...  data_analyst   \n",
       "1165  insurance analyst permanent dublin negotiable ...  data_analyst   \n",
       "\n",
       "                                         processed_text  label_encoded  \n",
       "1164  [role, operations, analysts, responsible, mana...              2  \n",
       "1165  [insurance, analyst, permanent, dublin, negoti...              2  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encode the 'label' column.\n",
    "df['label_encoded'] = df.label.map(lambda x: label2id[x.strip()])\n",
    "\n",
    "df.tail(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOTAL shape: (1166, 4)\n",
      "TRAINING shape: (816, 4)\n",
      "TEST shape: (350, 4)\n"
     ]
    }
   ],
   "source": [
    "train, test = train_test_split(df, test_size=0.3, random_state=630, stratify=df['label'])\n",
    "print('TOTAL shape:', df.shape)\n",
    "print('TRAINING shape:', train.shape)\n",
    "print('TEST shape:', test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences  = train['processed_text'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating new vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_model = Word2Vec(vector_size=300, min_count=1, epochs=10)\n",
    "ft_model.build_vocab(train_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before training my new model on my dataset, merging the vocabularies and injecting the pre-trained vectors. This step requires adjusting the initial weights for the overlapping vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m pretrained_model\u001b[38;5;241m.\u001b[39mkey_to_index:\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m ft_model\u001b[38;5;241m.\u001b[39mwv\u001b[38;5;241m.\u001b[39mkey_to_index:\n\u001b[1;32m----> 5\u001b[0m         ft_model\u001b[38;5;241m.\u001b[39mwv[word] \u001b[38;5;241m=\u001b[39m pretrained_model[word]\n",
      "File \u001b[1;32mc:\\Users\\temulenbd\\anaconda3\\Lib\\site-packages\\gensim\\models\\keyedvectors.py:635\u001b[0m, in \u001b[0;36mKeyedVectors.__setitem__\u001b[1;34m(self, keys, weights)\u001b[0m\n\u001b[0;32m    632\u001b[0m     keys \u001b[38;5;241m=\u001b[39m [keys]\n\u001b[0;32m    633\u001b[0m     weights \u001b[38;5;241m=\u001b[39m weights\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m--> 635\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_vectors(keys, weights, replace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\temulenbd\\anaconda3\\Lib\\site-packages\\gensim\\models\\keyedvectors.py:611\u001b[0m, in \u001b[0;36mKeyedVectors.add_vectors\u001b[1;34m(self, keys, weights, extras, replace)\u001b[0m\n\u001b[0;32m    609\u001b[0m \u001b[38;5;66;03m# change vectors, extras for in_vocab entities if `replace` flag is specified\u001b[39;00m\n\u001b[0;32m    610\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m replace:\n\u001b[1;32m--> 611\u001b[0m     in_vocab_idxs \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_index(keys[idx]) \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m np\u001b[38;5;241m.\u001b[39mnonzero(in_vocab_mask)[\u001b[38;5;241m0\u001b[39m]]\n\u001b[0;32m    612\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvectors[in_vocab_idxs] \u001b[38;5;241m=\u001b[39m weights[in_vocab_mask]\n\u001b[0;32m    613\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m attr, extra \u001b[38;5;129;01min\u001b[39;00m extras:\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mnonzero\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\temulenbd\\anaconda3\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:1888\u001b[0m, in \u001b[0;36m_nonzero_dispatcher\u001b[1;34m(a)\u001b[0m\n\u001b[0;32m   1884\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1885\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m asanyarray(a)\u001b[38;5;241m.\u001b[39mravel(order\u001b[38;5;241m=\u001b[39morder)\n\u001b[1;32m-> 1888\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_nonzero_dispatcher\u001b[39m(a):\n\u001b[0;32m   1889\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (a,)\n\u001b[0;32m   1892\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_nonzero_dispatcher)\n\u001b[0;32m   1893\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnonzero\u001b[39m(a):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ft_model.build_vocab([list(pretrained_model.key_to_index.keys())], update=True)\n",
    "ft_model.wv.vectors_lockf = np.ones(len(ft_model.wv))\n",
    "for word in pretrained_model.key_to_index:\n",
    "    if word in ft_model.wv.key_to_index:\n",
    "        ft_model.wv[word] = pretrained_model[word]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, I can train (fine-tune) the model on my dataset. This training will update the vectors for the new words and potentially adjust the vectors for the old words based on the new context provided by my data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1708640, 1708640)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft_model.train(train_sentences, total_examples=len(train_sentences), epochs=ft_model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_model.save('new_word2vec_temuulen')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec.load('new_word2vec_temuulen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_vec(words, model, num_features):\n",
    "    \"\"\"Average the word vectors for a set of words.\"\"\"\n",
    "    feature_vec = np.zeros((num_features,), dtype=\"float32\")\n",
    "    nwords = 0\n",
    "    index2word_set = set(model.wv.index_to_key)\n",
    "    \n",
    "    for word in words:\n",
    "        if word in index2word_set: \n",
    "            nwords += 1\n",
    "            feature_vec = np.add(feature_vec, model.wv[word])\n",
    "    \n",
    "    if nwords > 0:\n",
    "        feature_vec = np.divide(feature_vec, nwords)\n",
    "    return feature_vec\n",
    "\n",
    "def get_avg_feature_vecs(texts, model, num_features):\n",
    "    \"\"\"Calculate average feature vectors for all texts.\"\"\"\n",
    "    counter = 0\n",
    "    text_feature_vecs = np.zeros((len(texts), num_features), dtype=\"float32\")\n",
    "    \n",
    "    for text in texts:\n",
    "        text_feature_vecs[counter] = feature_vec(text, model, num_features)\n",
    "        counter += 1\n",
    "    \n",
    "    return text_feature_vecs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming `df` is your DataFrame and `model` is your Word2Vec model\n",
    "X = get_avg_feature_vecs(df['processed_text'], model, 300)  # 300 is the vector size of the Word2Vec model\n",
    "y = df['label_encoded'].values\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=630)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "classifier = LogisticRegression(max_iter=1000)\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "predictions = classifier.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, predictions))\n",
    "print(classification_report(y_test, predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_vector(word2vec_model, doc):\n",
    "    # Filter words in doc that are in the model's vocabulary\n",
    "    doc = [word for word in doc if word in word2vec_model.key_to_index]\n",
    "    if not doc:\n",
    "        return np.zeros(word2vec_model.vector_size)\n",
    "    return np.mean(word2vec_model[doc], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply feature extraction to the processed text\n",
    "X = np.array([document_vector(model, doc) for doc in df['processed_text']])\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract labels\n",
    "y = df['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the encoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Fit and transform labels to encode them\n",
    "Y = label_encoder.fit_transform(y)\n",
    "\n",
    "# Now `y_encoded` contains encoded labels suitable for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_temp, y_train, y_temp = train_test_split(X, Y, test_size=0.3, random_state=820, stratify=Y)\n",
    "X_validation, X_test, y_validation, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=820, stratify=y_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a logistic regression classifier\n",
    "classifier = LogisticRegression(random_state=630)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the classifier\n",
    "predictions = classifier.predict(X_validation)\n",
    "print(classification_report(y_validation, predictions))\n",
    "\n",
    "label_mapping = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\n",
    "\n",
    "print(\"Label Mapping:\")\n",
    "for label, encoded_num in label_mapping.items():\n",
    "    print(f\"{encoded_num}: {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump(classifier, 'ft_word2vec_temuulen')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Evaluating the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_model = load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttt = load('ft_word2vec_temuulen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = ttt.predict(X_test)\n",
    "print(classification_report(y_test, predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = y_test\n",
    "preds = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_labels = ['registered nurse', 'electrician', 'data analyst']\n",
    "cm_matrix = confusion_matrix(labels, preds)\n",
    "cm_title = \"CONFUSION MATRIX: fine-tuned 'bert-base-uncased' model for classification\"\n",
    "\n",
    "fig = ff.create_annotated_heatmap(z=cm_matrix, \n",
    "                                  x=cm_labels,\n",
    "                                  y=cm_labels, \n",
    "                                  colorscale='balance', \n",
    "                                  showscale=True,\n",
    "                                  annotation_text=cm_matrix)\n",
    "\n",
    "fig.update_layout(width=700, \n",
    "                  height=700, \n",
    "                  title=cm_title, \n",
    "                  title_x=0.5,\n",
    "                  xaxis=dict(title='Predicted Value', side='bottom'), \n",
    "                  yaxis_title='True Value')\n",
    "\n",
    "fig.update_yaxes(tickangle=-90)  \n",
    "    \n",
    "fig.show()\n",
    "\n",
    "# Print detailed classification report.\n",
    "report = classification_report(labels, preds, output_dict=True)\n",
    "report_title = \"CLASSIFICATION REPORT: fine-tuned 'bert-base-uncased' model for classification\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the text data from a .txt file\n",
    "benchmark_train = pd.read_csv('ag_news_train.txt', delimiter='\\t', header=None, names=['labels', 'text'])\n",
    "\n",
    "benchmark_test = pd.read_csv('ag_news_test.txt', delimiter='\\t', header=None, names=['labels', 'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_train['processed_text'] = benchmark_train['text'].apply(preprocess_text)\n",
    "\n",
    "benchmark_test['processed_text'] = benchmark_test['text'].apply(preprocess_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
