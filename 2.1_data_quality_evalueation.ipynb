{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT text classification: data validation of collected job description details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\temulenbd\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\temulenbd\\anaconda3\\Lib\\site-packages\\tensorflow_addons\\utils\\tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\temulenbd\\anaconda3\\Lib\\site-packages\\tensorflow_addons\\utils\\ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.12.0 and strictly below 2.15.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.15.0 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import DatasetDict, Dataset\n",
    "from transformers import TFAutoModel, AutoTokenizer\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "import tensorflow_addons as tfa\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_description</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>silver stream healthcare group offer great emp...</td>\n",
       "      <td>registered_nurse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>create a better future for yourself !! recruit...</td>\n",
       "      <td>registered_nurse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>access healthcare, one of ireland’s leading he...</td>\n",
       "      <td>registered_nurse</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     job_description             label\n",
       "0  silver stream healthcare group offer great emp...  registered_nurse\n",
       "1  create a better future for yourself !! recruit...  registered_nurse\n",
       "2  access healthcare, one of ireland’s leading he...  registered_nurse"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import finalized dataset as pandas data frame.\n",
    "df = pd.read_csv('data_jobads_final.csv', index_col=None)\n",
    "\n",
    "# Replace newline characters in the 'job_description' column with a space.\n",
    "df['job_description'] = df['job_description'].str.replace('\\n', ' ')\n",
    "\n",
    "# Remove None values.\n",
    "df = df.dropna()\n",
    "\n",
    "# Select and use only the last two columns for this evaluation.\n",
    "df = df.iloc[:,-2:]\n",
    "\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    644\n",
       "2    376\n",
       "1    146\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Updated 'label' column values according to the specified mapping.\n",
    "mapping = {'registered_nurse': 0, 'electrician': 1, 'data_analyst': 2}\n",
    "df['label_text'] = df['label']\n",
    "df['label'] = df['label'].replace(mapping) # or df['label'] = pd.factorize(df.label)[0]\n",
    "\n",
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the TRAINING dataset is: (816, 3)\n",
      "The shape of the VALIDATION dataset is: (350, 3)\n"
     ]
    }
   ],
   "source": [
    "# Split the DataFrame into training and testing sets while maintaining label proportions.\n",
    "train, validation = train_test_split(df, test_size=0.3, random_state=820, stratify=df['label'])\n",
    "\n",
    "print('The shape of the TRAINING dataset is:', train.shape)\n",
    "print('The shape of the VALIDATION dataset is:', validation.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert DataFrames to Hugging Face Dataset.\n",
    "train_dataset = Dataset.from_pandas(train)\n",
    "val_dataset = Dataset.from_pandas(validation)\n",
    "\n",
    "# Remove '__index_level_0__' feature\n",
    "train_dataset = train_dataset.remove_columns('__index_level_0__')\n",
    "val_dataset = val_dataset.remove_columns('__index_level_0__')\n",
    "\n",
    "# Create DatasetDict.\n",
    "jobads = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'validation': val_dataset,\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\temulenbd\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = TFAutoModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cc7901f340747279d45f6966a791a19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/816 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0b57c665734482ebd05ce01e117996f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/350 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize(batch):\n",
    "    return tokenizer(batch['job_description'], padding=True, truncation=True)\n",
    "\n",
    "jobads_encoded = jobads.map(tokenize, batched=True, batch_size=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['job_description', 'label', 'label_text', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 816\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['job_description', 'label', 'label_text', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 350\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jobads_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobads_encoded.set_format('tf',\n",
    "                          columns=['input_ids', 'attention_mask', 'token_type_ids', 'label'])\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "def order(inp):\n",
    "    '''\n",
    "    This function will group all the inputs of BERT into single\n",
    "    dictionary and then output it with labels.\n",
    "    '''\n",
    "    data = list(inp.values())\n",
    "    return {\n",
    "        'input_ids' : data[1],\n",
    "        'attention_mask' : data[2],\n",
    "        'token_type_ids' : data[3]\n",
    "    }, data[0]\n",
    "    \n",
    "# Convert train split of 'jobads_encoded' to tensorflow format.\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(jobads_encoded['train'][:])\n",
    "\n",
    "# Set batch_size and shuffle.\n",
    "train_dataset = train_dataset.batch(BATCH_SIZE).shuffle(100)\n",
    "\n",
    "# Map the 'order' function.\n",
    "train_dataset = train_dataset.map(order, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "# ... doing the same for test set ...\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices(jobads_encoded['validation'][:])\n",
    "val_dataset = val_dataset.batch(BATCH_SIZE)\n",
    "val_dataset = val_dataset.map(order, num_parallel_calls=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': <tf.Tensor: shape=(32, 512), dtype=int64, numpy=\n",
      "array([[  101,  3105,  6412, ...,     0,     0,     0],\n",
      "       [  101,  2057,  2024, ...,     0,     0,     0],\n",
      "       [  101, 15563,  2024, ...,     0,     0,     0],\n",
      "       ...,\n",
      "       [  101,  2307,  3105, ...,     0,     0,     0],\n",
      "       [  101, 15563,  2024, ...,     0,     0,     0],\n",
      "       [  101,  1996,  4456, ...,     0,     0,     0]], dtype=int64)>, 'attention_mask': <tf.Tensor: shape=(32, 512), dtype=int64, numpy=\n",
      "array([[0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)>, 'token_type_ids': <tf.Tensor: shape=(32, 512), dtype=int64, numpy=\n",
      "array([[1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0]], dtype=int64)>} \n",
      "\n",
      " tf.Tensor([0 0 0 0 2 0 2 0 0 0 2 0 0 2 0 0 0 0 2 0 2 1 0 0 0 2 2 0 2 0 0 0], shape=(32,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "inp, out = next(iter(train_dataset)) # a batch from train_dataset\n",
    "print(inp, '\\n\\n', out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTForClassification(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, bert_model, num_classes):\n",
    "        super().__init__()\n",
    "        self.bert = bert_model\n",
    "        self.fc = tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = self.bert(inputs)[1]\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + \n",
    "    K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = BERTForClassification(model, num_classes=3)\n",
    "\n",
    "classifier.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "    metrics=['accuracy', recall_m, precision_m, f1_m]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "WARNING:tensorflow:From c:\\Users\\temulenbd\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\temulenbd\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "26/26 [==============================] - 1780s 65s/step - loss: 0.8921 - accuracy: 0.6225 - recall_m: 1.2793 - precision_m: 0.6237 - f1_m: 0.8270\n",
      "Epoch 2/3\n",
      "26/26 [==============================] - 1654s 59s/step - loss: 0.5341 - accuracy: 0.8162 - recall_m: 1.0906 - precision_m: 0.4977 - f1_m: 0.6763\n",
      "Epoch 3/3\n",
      "26/26 [==============================] - 1463s 56s/step - loss: 0.1203 - accuracy: 0.9718 - recall_m: 1.0537 - precision_m: 0.4818 - f1_m: 0.6544\n"
     ]
    }
   ],
   "source": [
    "history = classifier.fit(\n",
    "    train_dataset,\n",
    "    epochs=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluation_results = classifier.evaluate(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'print(\\'All Metrics:\\')\\nfor metric_name, metric_value in zip(classifier.metrics_names, evaluation_results):\\n    print(f\"{metric_name}: {metric_value:.4f}\")'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''print('All Metrics:')\n",
    "for metric_name, metric_value in zip(classifier.metrics_names, evaluation_results):\n",
    "    print(f\"{metric_name}: {metric_value:.4f}\")'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
      "INFO:tensorflow:Assets written to: fine_tuned_bert_multi_label_classifier_temuulen\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: fine_tuned_bert_multi_label_classifier_temuulen\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n"
     ]
    }
   ],
   "source": [
    "classifier.save('fine_tuned_bert_multi_label_classifier_temuulen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"classifier.fit(train_dataset, epochs=3, validation_data=(val_data, val_labels))\\n\\n# After training, you can evaluate on the test set and get the confusion matrix\\ntest_loss, test_accuracy, test_precision, test_recall, test_f1 = classifier.evaluate(test_data, test_labels)\\nprint(f'Test Accuracy: {test_accuracy}')\\nprint(f'Test Precision: {test_precision}')\\nprint(f'Test Recall: {test_recall}')\\nprint(f'Test F1 Score: {test_f1}')\\n\\n# Get confusion matrix\\npredictions = classifier.predict(test_data)\\nconf_matrix = confusion_matrix(test_labels, np.argmax(predictions, axis=-1))\\nprint('Confusion Matrix:')\\nprint(conf_matrix)\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''classifier.fit(train_dataset, epochs=3, validation_data=(val_data, val_labels))\n",
    "\n",
    "# After training, you can evaluate on the test set and get the confusion matrix\n",
    "test_loss, test_accuracy, test_precision, test_recall, test_f1 = classifier.evaluate(test_data, test_labels)\n",
    "print(f'Test Accuracy: {test_accuracy}')\n",
    "print(f'Test Precision: {test_precision}')\n",
    "print(f'Test Recall: {test_recall}')\n",
    "print(f'Test F1 Score: {test_f1}')\n",
    "\n",
    "# Get confusion matrix\n",
    "predictions = classifier.predict(test_data)\n",
    "conf_matrix = confusion_matrix(test_labels, np.argmax(predictions, axis=-1))\n",
    "print('Confusion Matrix:')\n",
    "print(conf_matrix)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
