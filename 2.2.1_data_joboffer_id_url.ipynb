{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JOB OFFER'S DATA (part 1: extraction of the job id and url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Setting up for web scraping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load nessesary libraries.\n",
    "import time\n",
    "import selenium \n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "from time import sleep\n",
    "from random import randint\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define global variables.\n",
    "jobs = ['registered+nurse', 'electrician', 'data+analyst']\n",
    "job_titles = ['registered nurse', 'electrician', 'data analyst']\n",
    "job_list = []\n",
    "pagination_url = 'https://ie.indeed.com/jobs?q={}&l=Dublin%2C+County+Dublin&radius=25&filter=0&sort=date&start={}'\n",
    "max_iter_pgs = int()\n",
    "directory = r'C:\\Users\\temulenbd\\OneDrive\\Desktop\\learn\\github_repo\\cct\\capstone_project'\n",
    "df_name = ['df_rn', 'df_e', 'df_da']\n",
    "csv_file_name = ['data_jobads_rn.csv', 'data_jobads_e.csv', 'data_jobads_da.csv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom function that verifies how many positions are available for the specified job and how many pages can be iterated.\n",
    "def get_job_info(job_to_look, job_print):\n",
    "    \n",
    "    # Declare global variables.\n",
    "    global pagination_url\n",
    "    global max_iter_pgs\n",
    "    job = job_to_look\n",
    "    \n",
    "    # Set up Chrome webdriver options.\n",
    "    option= webdriver.ChromeOptions()\n",
    "    option.add_argument(\"--incognito\")\n",
    "    \n",
    "    # Specify the date.\n",
    "    current_date = datetime.now().date().strftime('%B %d, %Y')\n",
    "    start = time.time()\n",
    "    \n",
    "    # Initialize Chrome driver\n",
    "    driver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()), options=option)\n",
    "    driver.get(pagination_url.format(job, 0))\n",
    "\n",
    "    sleep(randint(4, 9))\n",
    "    job_number = driver.find_element(By.CLASS_NAME,'jobsearch-JobCountAndSortPane-jobCount').text\n",
    "    max_iter_pgs=int(job_number.split(' ')[0]) // 15 \n",
    "\n",
    "    # Close the WebDriver.\n",
    "    driver.quit()\n",
    "    end = time.time()\n",
    "\n",
    "    # Print results.\n",
    "    print(f'{job_print.upper()}')\n",
    "    print(f'Total number of vacancies available in Dublin area on {current_date}: {job_number}.')\n",
    "    print('Maximum number of iterable pages for the search:', max_iter_pgs, 'pages')\n",
    "    print('\\n')\n",
    "    print('Action was completed in:', end - start, 'seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a costum function that will extract data from the web-page and create a table with the information for the specified job.\n",
    "def scrape_job_details(job_to_look):\n",
    "    \n",
    "    # Declare global variables.\n",
    "    global max_iter_pgs\n",
    "    global job_list\n",
    "    global job_titles\n",
    "    global pagination_url\n",
    "    job =  job_to_look\n",
    "    \n",
    "    # Specify the date.\n",
    "    start = time.time()\n",
    "    \n",
    "    # Set up Chrome WebDriver.\n",
    "    driver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()))\n",
    "    sleep(randint(4, 9))\n",
    "\n",
    "    # Loop through each job posting through the pages and extract job details.\n",
    "    for i in range(0, max_iter_pgs):\n",
    "        driver.get(pagination_url.format(job, i * 10))\n",
    "        sleep(randint(4, 9))\n",
    "\n",
    "        job_page = driver.find_element(By.ID, 'mosaic-jobResults')\n",
    "        job_posts = job_page.find_elements(By.CLASS_NAME, 'job_seen_beacon')\n",
    "\n",
    "        for job_post in job_posts:\n",
    "            job_title = job_post.find_element(By.CLASS_NAME, 'jobTitle')\n",
    "            job_id = job_title.find_element(By.CSS_SELECTOR, 'a').get_attribute(\"id\")\n",
    "            job_link = job_title.find_element(By.CSS_SELECTOR, 'a').get_attribute(\"href\")\n",
    "\n",
    "            try:\n",
    "                job_date = job_post.find_element(By.CLASS_NAME, 'date').text\n",
    "            except Exception as e:\n",
    "                job_date = 'not available'\n",
    "\n",
    "            # Append job details to the job_list.\n",
    "            job_list.append([job_title.text, job_id, job_link, job_date])\n",
    "    \n",
    "    # Close the WebDriver.\n",
    "    driver.quit()\n",
    "    end = time.time()\n",
    "\n",
    "    # Check results.\n",
    "    print(f'{job.upper()} JOB ADS')\n",
    "    print(f'The total count of scraped job vacancies: {len(job_list)} jobs.\\n')\n",
    "    for x in range(min(2, len(job_list))):\n",
    "        print(f'JOB AD NO.{x + 1}:')\n",
    "        print(job_list[x][0])\n",
    "        print(job_list[x][1])\n",
    "        print(job_list[x][2])\n",
    "        print(job_list[x][3])\n",
    "    print('\\n')\n",
    "\n",
    "    print(f'The extraction was completed in: {(end - start) // 60} minutes and {(end - start) % 60} seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a costum function that creates a new DataFrame of given job title, transforms the data, exports it as a CSV file.\n",
    "def df_create_export_csv(new_df, csv):\n",
    "    \n",
    "    # Declare global variables.\n",
    "    global job_list\n",
    "    global directory\n",
    "    \n",
    "    # Create a new pandas Dataframe with the given job title.\n",
    "    column = ['title', 'id', 'link', 'date']\n",
    "    new_df = pd.DataFrame(job_list, columns=column)\n",
    "\n",
    "    # Loop through each row in the DataFrame for data transformation.\n",
    "    for x in range(int(new_df.shape[0])):\n",
    "        new_df.iat[x, 0] = new_df.iat[x, 0].lower()\n",
    "        new_df.iat[x, 3] = new_df.iat[x, 3].replace('Posted\\n', '')\n",
    "        new_df.iat[x, 0] = new_df.iat[x, 0].lower()\n",
    "\n",
    "    # Create the file path for CSV export.\n",
    "    file_path = os.path.join(directory, csv)\n",
    "    \n",
    "    # Export the DataFrame to CSV file.\n",
    "    new_df.to_csv(file_path, index=False)\n",
    "    \n",
    "    print(f\"The raw data was transformed and exported successfully as {file_path}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Web scraping\n",
    "**registered nurse ads**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REGISTERED NURSE\n",
      "Total number of vacancies available in Dublin area on January 10, 2024: 580 jobs.\n",
      "Maximum number of iterable pages for the search: 38 pages\n",
      "\n",
      "\n",
      "Action was completed in: 14.394474983215332 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Check the job availability on Indeed.com.\n",
    "get_job_info(jobs[0], job_titles[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REGISTERED+NURSE JOB ADS\n",
      "The total count of scraped job vacancies: 564 jobs.\n",
      "\n",
      "JOB AD NO.1:\n",
      "Assistant Director of Nursing\n",
      "sj_3c7e64c7996bb9d6\n",
      "https://ie.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0BI8IerbVtBNz9lxt-gGwgL1yqzg7rkL65oBt9kXiGchRfMu5HD70ENCeZDeMdLuF3Q6xzcJEOdJs3kZF1vPa7hsSvWDw387HGAhy09ybLyjuHgMbirifct4XvSysIpp9rS6Ba6AU4LkoNkAgvufXadgt9JcsvxlhJuDKpnvDOFeBo5dzQWo7pFgkc2ydoCMEtW2iWHYK8s0NBZGIKl4r7fRa5BSkndStcyPVvOs74vHKeTHQ7SUXy9Jvocb78gEg_dj47DKiNE4DkFJO0_gAKH8J7zdVJ2_SA0SWcKExPMVuj8cvElgzB1sNgtlr6Tj_tvJYbJguf9j_ZbW40QusqIW71ACZtcIEQtNDB2ACq92fxa2jeykoUuFHgXBCVybxWvDPNxEKC8ZR-CQ2U-uxIe24BspMOf_Q7ubq8y7Rn2TC_ww1c3qeBtg0-HG1nayqXJlu8_Q8JQxpi2Orw0JMwSvDbknG-Palp8oXVzt3wQSEGXv7POm22gus8PSdFTRhqn53K7WFof92B7UKnzoXDCOCUog7ovhXz-0jUTkR7jGF9BISOe6IYI7eSukOK5IeKuwIwAbEZoHgZeUmjLEFn634S8b_7FwUAq9oF0v_2kip74rvbHu1veWC9xeqPIVLR7LE43bTRiSpyNt1sryfSapz_GPLdBSSY=&xkcb=SoAG6_M3G50qpcR9Np0LbzkdCdPP&p=0&fvj=1&vjs=3\n",
      "Posted\n",
      "Just posted\n",
      "JOB AD NO.2:\n",
      "Clinical Nurse Manager (CNM)\n",
      "sj_358f1f68cde928c4\n",
      "https://ie.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0AVCuTJkMSq0gqIpm6bhV7v6tYBRH9u_MNnmp83dQl-I38w5fsjm0KOQH0nDZjSOcOPYeWFJgadE8QI-f8AsCzthoXfTX1WmFazIZi6tjV2ing9wXyxXhp7lrHTEwHGw33TyV5MyVmDISWer-boO4hTAqMhmFiAtf7IDTnpI23K40Jij9Q0j6aW2aOWk820si5mPPWVhm7AIkSwW8dHZlrys2oP-ctiwHu5cwgK7ciV5OPb7h0mpWne8TXN-KHFWIZln_ehdQkSNO2_tZ7aoHStdDhELSKkzN7n0C3WtaW--k8LwviHm8BaMv07tLVWjGGLUNQ9hR_-tZ8imUhykD44K4RQ-ZndbqRvovo6zKTRGs7iY8lYX1UXgOFQLyPzJ5hhE0gfRllgdkKsd3IbKwd4HnS7gi9ZL55Awuh7XRmQPb_fYZPNJ-OvuflX2z22DL_EGrTcnFNl5VA-ZR52eEnqq9JktL8cmhMQbsCpLROzSsF2eS9A5ILOoEBR_A2ECD3mGLo-FvcVgx9-bBVyG9Ch-GmlSFUg4PALHicwMkW4_7nHUVblakfQAD0znGLIefY9CGJYbHyxSKOzhkyHYLW6jIqcjUmLBV98LHmVdNU0hU_Dtuq5BbXVtY1pmcRPQ1qxzSDuPIpO9FkhSUFnTLpY-QfpAirJSoQ=&xkcb=SoCy6_M3G50qpcR9Np0KbzkdCdPP&p=1&fvj=1&vjs=3\n",
      "Hiring ongoing\n",
      "\n",
      "\n",
      "The extraction was completed in: 5.0 minutes and 22.88646674156189 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Scrape the data available on Indeed.com and save into 'job_list' variable.\n",
    "scrape_job_details(jobs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The raw data was transformed and exported successfully as C:\\Users\\temulenbd\\OneDrive\\Desktop\\learn\\github_repo\\cct\\capstone_project\\data_jobads_rn.csv.\n"
     ]
    }
   ],
   "source": [
    "# Transform the scraped data and export it as a CSV file.\n",
    "df_create_export_csv(df_name[0], csv_file_name[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset variables.\n",
    "max_iter_pgs = int()\n",
    "job_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**electrician ads**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ELECTRICIAN\n",
      "Total number of vacancies available in Dublin area on January 10, 2024: 166 jobs.\n",
      "Maximum number of iterable pages for the search: 11 pages\n",
      "\n",
      "\n",
      "Action was completed in: 15.139153242111206 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Check the job availability on Indeed.com.\n",
    "get_job_info(jobs[1], job_titles[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ELECTRICIAN JOB ADS\n",
      "The total count of scraped job vacancies: 155 jobs.\n",
      "\n",
      "JOB AD NO.1:\n",
      "Industrial Electrician\n",
      "sj_7cba7a465e6641fe\n",
      "https://ie.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0AnEYSss4eDbTLySY2p2efwzP3CilC5Xfeyf166GCcN9JJMj_EWM2A3xodspN8Pi50AQZK4e0X589pQszonXkMkH0RQ88koQVn7ZT1ssPBpQ_NkkqQhHLy474KArTvD5gBqTeIgUZnCJu_dKf5oe4HAkKcLEP2oyZYldxw_5f6gvn14xcRPMRza8-QyIqzv0byDd4PvDXXzJE4qOMdLC5mFAHmK1MYBMw5p7DiJwb-0l9Wn2nPXXR0fXfDw3ZGgpTK2fEod5hefk8SHbbBTg5XXqH-cqkCUwZQ9x2EWDy0u4UzQIublT1xvmjgu6FKChkkSwYiOeLdBpClFaBomH_21iQ0g5LHJAjhpQKbe5Rsk1qWO2CAoxB8uzG0hGry5nE_Kk10xn7b1s6IhQqZOo55YeLj-GzbQ2w-DTtb3Sp4X3C8eZfASRrQDZn5dl8gS8f63CC-Vla0OSTSqrKEexz5CHQ9huC6i-HpqLL5cp0JRmXoTInszOBMVKdAONBilh2S8z-AKr5sldSdsgOg8u-rA4qQggC-Z_S6yGMKpGlEqi3SmevB2y9AV74UJrhYkGLBICcE3LTwSgJcgH953pBiTYB-Ykz2oCw3HwOcSIklmmHLJtX1pEGpD9CH-Kc449-wE79tKz3lqJA==&xkcb=SoBz6_M3G51AfAR9Np0LbzkdCdPP&p=0&fvj=1&vjs=3\n",
      "Posted\n",
      "Just posted\n",
      "JOB AD NO.2:\n",
      "Apprentice Electrician\n",
      "job_a6937c5385359d51\n",
      "https://ie.indeed.com/rc/clk?jk=a6937c5385359d51&bb=j7vMUYt2-bi_4MmLfG7BPwbndCrrWJzJhXCu32Q571yx9H_OeNsgAGawfb_SkW2e4AVoT8Q2rWmlax6AWUD7itP5hv9nZ_uvuhRkI_-Y07E%3D&xkcb=SoAz67M3G51AfAR9Np0KbzkdCdPP&fccid=c021a655a4b9e800&cmp=STS-Group&ti=Apprentice+Electrician&vjs=3\n",
      "Posted\n",
      "Just posted\n",
      "\n",
      "\n",
      "The extraction was completed in: 1.0 minutes and 43.00874090194702 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Scrape the data available on Indeed.com and save into 'job_list' variable.\n",
    "scrape_job_details(jobs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The raw data was transformed and exported successfully as C:\\Users\\temulenbd\\OneDrive\\Desktop\\learn\\github_repo\\cct\\capstone_project\\data_jobads_e.csv.\n"
     ]
    }
   ],
   "source": [
    "# Transform the scraped data and export it as a CSV file.\n",
    "df_create_export_csv(df_name[1], csv_file_name[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset variables.\n",
    "max_iter_pgs = int()\n",
    "job_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**data analyst ads**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA ANALYST\n",
      "Total number of vacancies available in Dublin area on January 10, 2024: 342 jobs.\n",
      "Maximum number of iterable pages for the search: 22 pages\n",
      "\n",
      "\n",
      "Action was completed in: 12.003286123275757 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Check the job availability on Indeed.com.\n",
    "get_job_info(jobs[2], job_titles[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA+ANALYST JOB ADS\n",
      "The total count of scraped job vacancies: 315 jobs.\n",
      "\n",
      "JOB AD NO.1:\n",
      "Inventory Analyst\n",
      "sj_d72c4cf42121ca22\n",
      "https://ie.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0AnxfxssoujI3GvYsxvmNCzxzm6VnPtwq4vmbotio0JvQP4qXO3kiNsl1yUmVNF84MzBzmk8-uNcalZeVGMR2tlbw0j0ghDB32zhGcWEZiFsh6Zhjvcky-uhasSq90-Q5tQuwoeDbw8SltNynq_LNJvZUMRCpbvjFC48w4dMABSo4ebojJMmZ--dHZHBiOPE135_T6Xa7ud4X-cfbHmQ6gOKJv1AWhNFU4c5f5OSZMb91QEb925KxbCkocbzbpn9dQx3uhvO9W8KGpcJhKtOfLETuEJsfe-UuHiydeNE0qvY3PL0wvF7dhZ_XawpnVmj6C9DhBC5cnxriFAefUXHrWNQmj-WT9vVpoL95aOJdIc7s7y1ypjJI3yNWLyvdOVs6umuVRUmwN_uqs6ePa9Dc4h8SHkPM_GPMXsZLziJ3wN7iIsmpT6cHqk2BA4lWfDKw0S5d6TckE8Xzv-8UyHiDSQaOsejKEjKLi1uYZCRQAnHUSP5nZLrZdqQ18ZggJQSwBOJNIXuo3CsOYtW0UYGcMiBfWpdPuCYCdmtzEAcKghJZr_S1MYHgV56H-H6wpXyZ44S6SmdX4gn64qpFPnTP6sWpx5E6Pd6TpjCYvf9GoRuaeJDPvU4xDHpCQpEE2ih4frPZwuYx3gDPR5XMbklqnoNQaAOZZ4X18oOwIhhmHa4kVpQor0_f9mKTUHw5oxri0AkpEu-qcHoQ==&xkcb=SoDk6_M3G51RjlQHEx0LbzkdCdPP&p=0&fvj=0&vjs=3\n",
      "Posted\n",
      "Just posted\n",
      "JOB AD NO.2:\n",
      "Defect Control Engineer with Strong data analysis and programing skills (SQL/Python/MATLAB).\n",
      "job_9d518bcdd2dc8ca0\n",
      "https://ie.indeed.com/rc/clk?jk=9d518bcdd2dc8ca0&bb=WFqqXi9ONJgXrTWGjxsH4_o-jPUX29-mPu0LCwUqhMc0TNGLxpap4cREZ8pq4yGwglpravmc3kDvRNT5iTj29WA89UGPwrWw-ZQBSBmS-ss%3D&xkcb=SoCk67M3G51RjlQHEx0KbzkdCdPP&fccid=936367796261bd6e&vjs=3\n",
      "Posted\n",
      "Just posted\n",
      "\n",
      "\n",
      "The extraction was completed in: 2.0 minutes and 59.2672963142395 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Scrape the data available on Indeed.com and save into 'job_list' variable.\n",
    "scrape_job_details(jobs[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The raw data was transformed and exported successfully as C:\\Users\\temulenbd\\OneDrive\\Desktop\\learn\\github_repo\\cct\\capstone_project\\data_jobads_da.csv.\n"
     ]
    }
   ],
   "source": [
    "# Transform the scraped data and export it as a CSV file.\n",
    "df_create_export_csv(df_name[2], csv_file_name[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
