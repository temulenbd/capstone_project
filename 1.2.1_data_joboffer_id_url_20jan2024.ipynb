{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JOB OFFER'S DATA (part 1: extraction of the job id and url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Setting up for web scraping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load nessesary libraries.\n",
    "import time\n",
    "import selenium \n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "from time import sleep\n",
    "from random import randint\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define global variables.\n",
    "jobs = ['registered+nurse', 'electrician', 'data+analyst']\n",
    "job_titles = ['registered nurse', 'electrician', 'data analyst']\n",
    "job_list = []\n",
    "pagination_url = 'https://ie.indeed.com/jobs?q={}&l=Dublin%2C+County+Dublin&radius=25&filter=0&sort=date&start={}'\n",
    "max_iter_pgs = int()\n",
    "directory = r'C:\\Users\\temulenbd\\OneDrive\\Desktop\\learn\\github_repo\\cct\\capstone_project'\n",
    "df_name = ['df_rn', 'df_e', 'df_da']\n",
    "csv_file_name = ['data_jobads_rn_20jan.csv', 'data_jobads_e_20jan.csv', 'data_jobads_da_20jan.csv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom function that verifies how many positions are available for the specified job and how many pages can be iterated.\n",
    "def get_job_info(job_to_look, job_print):\n",
    "    \n",
    "    # Declare global variables.\n",
    "    global pagination_url\n",
    "    global max_iter_pgs\n",
    "    job = job_to_look\n",
    "    \n",
    "    # Set up Chrome webdriver options.\n",
    "    option= webdriver.ChromeOptions()\n",
    "    option.add_argument(\"--incognito\")\n",
    "    \n",
    "    # Specify the date.\n",
    "    current_date = datetime.now().date().strftime('%B %d, %Y')\n",
    "    start = time.time()\n",
    "    \n",
    "    # Initialize Chrome driver\n",
    "    driver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()), options=option)\n",
    "    driver.get(pagination_url.format(job, 0))\n",
    "\n",
    "    sleep(randint(4, 9))\n",
    "    job_number = driver.find_element(By.CLASS_NAME,'jobsearch-JobCountAndSortPane-jobCount').text\n",
    "    max_iter_pgs=int(job_number.split(' ')[0]) // 15 \n",
    "\n",
    "    # Close the WebDriver.\n",
    "    driver.quit()\n",
    "    end = time.time()\n",
    "\n",
    "    # Print results.\n",
    "    print(f'{job_print.upper()}')\n",
    "    print(f'Total number of vacancies available in Dublin area on {current_date}: {job_number}.')\n",
    "    print('Maximum number of iterable pages for the search:', max_iter_pgs, 'pages')\n",
    "    print('\\n')\n",
    "    print('Action was completed in:', end - start, 'seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a costum function that will extract data from the web-page and create a table with the information for the specified job.\n",
    "def scrape_job_details(job_to_look):\n",
    "    \n",
    "    # Declare global variables.\n",
    "    global max_iter_pgs\n",
    "    global job_list\n",
    "    global job_titles\n",
    "    global pagination_url\n",
    "    job =  job_to_look\n",
    "    \n",
    "    # Specify the date.\n",
    "    start = time.time()\n",
    "    \n",
    "    # Set up Chrome WebDriver.\n",
    "    driver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()))\n",
    "    sleep(randint(4, 9))\n",
    "\n",
    "    # Loop through each job posting through the pages and extract job details.\n",
    "    for i in range(0, max_iter_pgs):\n",
    "        driver.get(pagination_url.format(job, i * 10))\n",
    "        sleep(randint(4, 9))\n",
    "\n",
    "        job_page = driver.find_element(By.ID, 'mosaic-jobResults')\n",
    "        job_posts = job_page.find_elements(By.CLASS_NAME, 'job_seen_beacon')\n",
    "\n",
    "        for job_post in job_posts:\n",
    "            job_title = job_post.find_element(By.CLASS_NAME, 'jobTitle')\n",
    "            job_id = job_title.find_element(By.CSS_SELECTOR, 'a').get_attribute(\"id\")\n",
    "            job_link = job_title.find_element(By.CSS_SELECTOR, 'a').get_attribute(\"href\")\n",
    "\n",
    "            try:\n",
    "                job_date = job_post.find_element(By.CLASS_NAME, 'date').text\n",
    "            except Exception as e:\n",
    "                job_date = 'not available'\n",
    "\n",
    "            # Append job details to the job_list.\n",
    "            job_list.append([job_title.text, job_id, job_link, job_date])\n",
    "    \n",
    "    # Close the WebDriver.\n",
    "    driver.quit()\n",
    "    end = time.time()\n",
    "\n",
    "    # Check results.\n",
    "    print(f'{job.upper()} JOB ADS')\n",
    "    print(f'The total count of scraped job vacancies: {len(job_list)} jobs.\\n')\n",
    "    for x in range(min(2, len(job_list))):\n",
    "        print(f'JOB AD NO.{x + 1}:')\n",
    "        print(job_list[x][0])\n",
    "        print(job_list[x][1])\n",
    "        print(job_list[x][2])\n",
    "        print(job_list[x][3])\n",
    "    print('\\n')\n",
    "\n",
    "    print(f'The extraction was completed in: {(end - start) // 60} minutes and {(end - start) % 60} seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a costum function that creates a new DataFrame of given job title, transforms the data, exports it as a CSV file.\n",
    "def df_create_export_csv(new_df, csv):\n",
    "    \n",
    "    # Declare global variables.\n",
    "    global job_list\n",
    "    global directory\n",
    "    \n",
    "    # Create a new pandas Dataframe with the given job title.\n",
    "    column = ['title', 'id', 'link', 'date']\n",
    "    new_df = pd.DataFrame(job_list, columns=column)\n",
    "\n",
    "    # Loop through each row in the DataFrame for data transformation.\n",
    "    for x in range(int(new_df.shape[0])):\n",
    "        new_df.iat[x, 0] = new_df.iat[x, 0].lower()\n",
    "        new_df.iat[x, 3] = new_df.iat[x, 3].replace('Posted\\n', '')\n",
    "        new_df.iat[x, 0] = new_df.iat[x, 0].lower()\n",
    "\n",
    "    # Create the file path for CSV export.\n",
    "    file_path = os.path.join(directory, csv)\n",
    "    \n",
    "    # Export the DataFrame to CSV file.\n",
    "    new_df.to_csv(file_path, index=False)\n",
    "    \n",
    "    print(f\"The raw data was transformed and exported successfully as {file_path}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Web scraping\n",
    "**registered nurse ads**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REGISTERED NURSE\n",
      "Total number of vacancies available in Dublin area on January 20, 2024: 625 jobs.\n",
      "Maximum number of iterable pages for the search: 41 pages\n",
      "\n",
      "\n",
      "Action was completed in: 14.182044982910156 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Check the job availability on Indeed.com.\n",
    "get_job_info(jobs[0], job_titles[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REGISTERED+NURSE JOB ADS\n",
      "The total count of scraped job vacancies: 612 jobs.\n",
      "\n",
      "JOB AD NO.1:\n",
      "Cardiac Staff Nurse\n",
      "job_1df6cdf12a7ff3b4\n",
      "https://ie.indeed.com/rc/clk?jk=1df6cdf12a7ff3b4&bb=KDvhOqIgqZ5NSFT5QjrOO23C8PptS_z7nVKCwpR56LoW4Fzx2imYf-7VMaZlwzzxoISVquSka--EG3V8ehIRKA5OjDsy4MsGmD2jxDyABCk%3D&xkcb=SoBL67M3FTSZ_k07Kh0LbzkdCdPP&fccid=c6715a18e860f1f6&cmp=White-Label-Management&ti=Cardiac+Nurse&vjs=3\n",
      "Posted\n",
      "Today\n",
      "JOB AD NO.2:\n",
      "Theatre Staff Nurse\n",
      "job_8dac085957f00f90\n",
      "https://ie.indeed.com/rc/clk?jk=8dac085957f00f90&bb=KDvhOqIgqZ5NSFT5QjrOO6vLP6RzbBCm65ZCHQdoTmzEiqePkoPCU2j6-hqwGtfBBeRpxgNS3TG6UM9DKg08LpgcfZy1hPi48cPuJnbG5Go%3D&xkcb=SoD_67M3FTSZ_k07Kh0KbzkdCdPP&fccid=c6715a18e860f1f6&cmp=White-Label-Management&ti=Theatre+Nurse&vjs=3\n",
      "Posted\n",
      "Today\n",
      "\n",
      "\n",
      "The extraction was completed in: 5.0 minutes and 17.33529567718506 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Scrape the data available on Indeed.com and save into 'job_list' variable.\n",
    "scrape_job_details(jobs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The raw data was transformed and exported successfully as C:\\Users\\temulenbd\\OneDrive\\Desktop\\learn\\github_repo\\cct\\capstone_project\\data_jobads_rn_20jan.csv.\n"
     ]
    }
   ],
   "source": [
    "# Transform the scraped data and export it as a CSV file.\n",
    "df_create_export_csv(df_name[0], csv_file_name[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset variables.\n",
    "max_iter_pgs = int()\n",
    "job_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**electrician ads**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ELECTRICIAN\n",
      "Total number of vacancies available in Dublin area on January 20, 2024: 151 jobs.\n",
      "Maximum number of iterable pages for the search: 10 pages\n",
      "\n",
      "\n",
      "Action was completed in: 16.498794078826904 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Check the job availability on Indeed.com.\n",
    "get_job_info(jobs[1], job_titles[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ELECTRICIAN JOB ADS\n",
      "The total count of scraped job vacancies: 145 jobs.\n",
      "\n",
      "JOB AD NO.1:\n",
      "Maintenance Technician (Electrical) - Dublin 3 (AM18158)\n",
      "sj_8406b1af34f03d2b\n",
      "https://ie.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0D8963UVSHQvE14Gz87xEjFAazKwo8pwGt2AcCk0nHr42kDSuf3Mi8ZyaVg53Qijir90_Qm1Q588QQ6lWUwXDJ9nXC3sedbrJN4vxMujoOrQL0P5BIeuVMeMEdWqKYoybDsEv-NLXMbDBTSK3axGohD5OOQrNi0BrvJrdlam0i1_kdtCLwFtT0UwVETkTSJlFdYUqkYowxlhmhAAj-DGUAn7ppL5nftu46BXs_hX00-9myD5T9LcnIx0xATWT8kBCkAa9MKK5MVU73xK5joXIIrlK4_owpuEg6_dglcR6u1G9NK9ZcNoXIjFV_vw7Uo4_IHMJiqhZYm7LMtoUx3ufx8oQy9ThJmsA7VBhClDeIpp8xhycYf4v5k-EJpZMRRqatfYIwToH6y777i3bX6gAwoMwpv7xgE-c_FxeGCt2b2oLnnll1S7XHF09wGR4NK3vwGgitHCOBZ_gbYOjei-V6sAz3rlads2XRQM5oZm7RxVje364Sk_9DZGjkx2w3APZDc_9giWsD5eLo_v6HYUzj24SuUMZpbjnAcJQ1t0JNstpTSIUm4z1Ua68G8dxFp6KYBWx0eThN_BgxtUGIJq4WfUmYQmdk0yOsnpwLehOHXJeYIK5CTNfixZKZy3a8wFJJLnsAjCnq-YCMcIMhT6q50dxVjdvBwjvrSlso9yXZDN3bd1HRBIk0Z&xkcb=SoC86_M3FTSwrK07Mx0LbzkdCdPP&p=0&fvj=1&vjs=3\n",
      "Posted\n",
      "Today\n",
      "JOB AD NO.2:\n",
      "Electrician\n",
      "sj_90090379dc44bbc5\n",
      "https://ie.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0BAkp0XFzye14tYByJ9bG7_G8t2H0RsHXoy06gAIxoqHWudm3mQ_GjHMQrQzWg9cjU1crEor7oYuzvKPNPvmQmUuv2HBodb45Kud5RTh-pPT6lmKekqLMKKhywhGj7ELfGmlX7XKlDdhMLSCbY9GqT0lqNrBPBsfxxrUg1mrjq1Bxiv19mpk3gVcd3rqNsJMRVWD4KbYh_MSrDhFI_DpzQD35whKdRrPY3nK-ixVnT9m-NpaVivaXF3SZ4atPvTq0XiOukJMGwEiScm4IqWluaR9om-b9Ys3Ok-Swm0GJxqKwegtchzkhVmVPenOgzuN_HWQnMZ_GEVkx7iifo4eejrPGHxIaFPCcyT44wgypSQ5vtCS3kmxwsSefzmkEX7dFwygZAmZRzrDEBD9J3-53ciED24iVwMBR_RDiXKCEqZfzYBDCIT6w8bTDvjhFcFjTyfNTxmWP0A5K7R7wGiw-QX2tSZoAz2z4We6pkPsTWXuaSQja-veZNIefNyVDduNeozjcPazAYilewUdKYx3F1fq_4bpUyM4mFK83wMDatVSIqQBgVXjx1Pt520Rd5YQg2KoqVe8yqoK5tCS7NBSNkyqbRryj_0dAR18igLToOI-3APqArignoxU0S2PfiU4Lw=&xkcb=SoAI6_M3FTSwrK07Mx0KbzkdCdPP&p=1&fvj=1&vjs=3\n",
      "Employer\n",
      "Active 2 days ago\n",
      "\n",
      "\n",
      "The extraction was completed in: 1.0 minutes and 29.19952654838562 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Scrape the data available on Indeed.com and save into 'job_list' variable.\n",
    "scrape_job_details(jobs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The raw data was transformed and exported successfully as C:\\Users\\temulenbd\\OneDrive\\Desktop\\learn\\github_repo\\cct\\capstone_project\\data_jobads_e_20jan.csv.\n"
     ]
    }
   ],
   "source": [
    "# Transform the scraped data and export it as a CSV file.\n",
    "df_create_export_csv(df_name[1], csv_file_name[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset variables.\n",
    "max_iter_pgs = int()\n",
    "job_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**data analyst ads**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA ANALYST\n",
      "Total number of vacancies available in Dublin area on January 20, 2024: 346 jobs.\n",
      "Maximum number of iterable pages for the search: 23 pages\n",
      "\n",
      "\n",
      "Action was completed in: 17.141117811203003 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Check the job availability on Indeed.com.\n",
    "get_job_info(jobs[2], job_titles[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA+ANALYST JOB ADS\n",
      "The total count of scraped job vacancies: 334 jobs.\n",
      "\n",
      "JOB AD NO.1:\n",
      "Data Analyst\n",
      "job_3ffe8294304602bd\n",
      "https://ie.indeed.com/rc/clk?jk=3ffe8294304602bd&bb=Lda2kDTxpK_Bw6ZiyREAuz9Bque7xHJNVLFtjAnziN5Rikp1PLpDzD63vAnfHjd2GGsoOHslDLTNc1jHq4XAFOv-nHBFjj1w299F_1-gEsA%3D&xkcb=SoC367M3FTTCt1WHMZ0LbzkdCdPP&fccid=e74773ca4b4eccf9&vjs=3\n",
      "Posted\n",
      "Just posted\n",
      "JOB AD NO.2:\n",
      "Data Analyst\n",
      "job_84870afff879430a\n",
      "https://ie.indeed.com/rc/clk?jk=84870afff879430a&bb=Lda2kDTxpK_Bw6ZiyREAu1JmhJayD93Zna4KCNZPIeMyOckg4Cld4HDuA1FJ7Bl4ftI0c8POrW2ax59bVi2U8_kCNm04f0fL7UBE3IYXDWg%3D&xkcb=SoAD67M3FTTCt1WHMZ0KbzkdCdPP&fccid=855f48b961808012&vjs=3\n",
      "Posted\n",
      "Today\n",
      "\n",
      "\n",
      "The extraction was completed in: 3.0 minutes and 3.578531503677368 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Scrape the data available on Indeed.com and save into 'job_list' variable.\n",
    "scrape_job_details(jobs[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The raw data was transformed and exported successfully as C:\\Users\\temulenbd\\OneDrive\\Desktop\\learn\\github_repo\\cct\\capstone_project\\data_jobads_da_20jan.csv.\n"
     ]
    }
   ],
   "source": [
    "# Transform the scraped data and export it as a CSV file.\n",
    "df_create_export_csv(df_name[2], csv_file_name[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
