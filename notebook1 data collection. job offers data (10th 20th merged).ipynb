{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TEMUULEN Bulgan\n",
    "### [A COMPARATIVE EVALUATION OF TEXT REPRESENTATION TECHNIQUES FOR CONTENT-BASED JOB RECOMMENDATION SYSTEM](https://github.com/temulenbd/jrs)\n",
    "#### `PART I: JOB OFFERS' DATASET` \n",
    "#### *This part of the project includes the coding for collecting the job offers dataset through web scraping and the subsequent processing of the collected data.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I.I Job offers' data collection (web scraping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10th JAN, 2024. extraction of the job ID and URL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Setting up for web scraping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MODULE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load nessesary libraries.\n",
    "import time\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "from time import sleep\n",
    "from random import randint\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GLOBAL VARIABLE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define global variables.\n",
    "jobs = ['registered+nurse', 'electrician', 'data+analyst']\n",
    "job_titles = ['registered nurse', 'electrician', 'data analyst']\n",
    "job_list = []\n",
    "pagination_url = 'https://ie.indeed.com/jobs?q={}&l=Dublin%2C+County+Dublin&radius=25&filter=0&sort=date&start={}'\n",
    "max_iter_pgs = int()\n",
    "directory = r'C:\\Users\\temulenbd\\OneDrive\\Desktop\\learn\\github_repo\\cct\\capstone_project'\n",
    "df_name = ['df_rn', 'df_e', 'df_da']\n",
    "csv_file_name = ['data_jobads_rn.csv', 'data_jobads_e.csv', 'data_jobads_da.csv']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Web scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PREPARATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom function that verifies how many positions are available for the specified job and how many pages can be iterated.\n",
    "def get_job_info(job_to_look, job_print):\n",
    "    \n",
    "    # Declare global variables.\n",
    "    global pagination_url\n",
    "    global max_iter_pgs\n",
    "    job = job_to_look\n",
    "    \n",
    "    # Set up Chrome webdriver options.\n",
    "    option= webdriver.ChromeOptions()\n",
    "    option.add_argument(\"--incognito\")\n",
    "    \n",
    "    # Specify the date.\n",
    "    current_date = datetime.now().date().strftime('%B %d, %Y')\n",
    "    start = time.time()\n",
    "    \n",
    "    # Initialize Chrome driver\n",
    "    driver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()), options=option)\n",
    "    driver.get(pagination_url.format(job, 0))\n",
    "\n",
    "    sleep(randint(4, 9))\n",
    "    job_number = driver.find_element(By.CLASS_NAME,'jobsearch-JobCountAndSortPane-jobCount').text\n",
    "    max_iter_pgs=int(job_number.split(' ')[0]) // 15 \n",
    "\n",
    "    # Close the WebDriver.\n",
    "    driver.quit()\n",
    "    end = time.time()\n",
    "\n",
    "    # Print results.\n",
    "    print(f'{job_print.upper()}')\n",
    "    print(f'Total number of vacancies available in Dublin area on {current_date}: {job_number}.')\n",
    "    print('Maximum number of iterable pages for the search:', max_iter_pgs, 'pages')\n",
    "    print('\\n')\n",
    "    print('Action was completed in:', end - start, 'seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a costum function that will extract data from the web-page and create a table with the information for the specified job.\n",
    "def scrape_job_details(job_to_look):\n",
    "    \n",
    "    # Declare global variables.\n",
    "    global max_iter_pgs\n",
    "    global job_list\n",
    "    global job_titles\n",
    "    global pagination_url\n",
    "    job =  job_to_look\n",
    "    \n",
    "    # Specify the date.\n",
    "    start = time.time()\n",
    "    \n",
    "    # Set up Chrome WebDriver.\n",
    "    driver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()))\n",
    "    sleep(randint(4, 9))\n",
    "\n",
    "    # Loop through each job posting through the pages and extract job details.\n",
    "    for i in range(0, max_iter_pgs):\n",
    "        driver.get(pagination_url.format(job, i * 10))\n",
    "        sleep(randint(4, 9))\n",
    "\n",
    "        job_page = driver.find_element(By.ID, 'mosaic-jobResults')\n",
    "        job_posts = job_page.find_elements(By.CLASS_NAME, 'job_seen_beacon')\n",
    "\n",
    "        for job_post in job_posts:\n",
    "            job_title = job_post.find_element(By.CLASS_NAME, 'jobTitle')\n",
    "            job_id = job_title.find_element(By.CSS_SELECTOR, 'a').get_attribute(\"id\")\n",
    "            job_link = job_title.find_element(By.CSS_SELECTOR, 'a').get_attribute(\"href\")\n",
    "\n",
    "            try:\n",
    "                job_date = job_post.find_element(By.CLASS_NAME, 'date').text\n",
    "            except Exception as e:\n",
    "                job_date = 'not available'\n",
    "\n",
    "            # Append job details to the job_list.\n",
    "            job_list.append([job_title.text, job_id, job_link, job_date])\n",
    "    \n",
    "    # Close the WebDriver.\n",
    "    driver.quit()\n",
    "    end = time.time()\n",
    "\n",
    "    # Check results.\n",
    "    print(f'{job.upper()} JOB ADS')\n",
    "    print(f'The total count of scraped job vacancies: {len(job_list)} jobs.\\n')\n",
    "    for x in range(min(2, len(job_list))):\n",
    "        print(f'JOB AD NO.{x + 1}:')\n",
    "        print(job_list[x][0])\n",
    "        print(job_list[x][1])\n",
    "        print(job_list[x][2])\n",
    "        print(job_list[x][3])\n",
    "    print('\\n')\n",
    "\n",
    "    print(f'The extraction was completed in: {(end - start) // 60} minutes and {(end - start) % 60} seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a costum function that creates a new DataFrame of given job title, transforms the data, exports it as a CSV file.\n",
    "def df_create_export_csv(new_df, csv):\n",
    "    \n",
    "    # Declare global variables.\n",
    "    global job_list\n",
    "    global directory\n",
    "    \n",
    "    # Create a new pandas Dataframe with the given job title.\n",
    "    column = ['title', 'id', 'link', 'date']\n",
    "    new_df = pd.DataFrame(job_list, columns=column)\n",
    "\n",
    "    # Loop through each row in the DataFrame for data transformation.\n",
    "    for x in range(int(new_df.shape[0])):\n",
    "        new_df.iat[x, 0] = new_df.iat[x, 0].lower()\n",
    "        new_df.iat[x, 3] = new_df.iat[x, 3].replace('Posted\\n', '')\n",
    "        new_df.iat[x, 0] = new_df.iat[x, 0].lower()\n",
    "\n",
    "    # Create the file path for CSV export.\n",
    "    file_path = os.path.join(directory, csv)\n",
    "    \n",
    "    # Export the DataFrame to CSV file.\n",
    "    # new_df.to_csv(file_path, index=False)\n",
    "    \n",
    "    print(f\"The raw data was transformed and exported successfully as {file_path}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SCRAPING**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*registered nurse ads*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REGISTERED NURSE\n",
      "Total number of vacancies available in Dublin area on January 10, 2024: 580 jobs.\n",
      "Maximum number of iterable pages for the search: 38 pages\n",
      "\n",
      "\n",
      "Action was completed in: 14.394474983215332 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Check the job availability on Indeed.com.\n",
    "get_job_info(jobs[0], job_titles[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REGISTERED+NURSE JOB ADS\n",
      "The total count of scraped job vacancies: 564 jobs.\n",
      "\n",
      "JOB AD NO.1:\n",
      "Assistant Director of Nursing\n",
      "sj_3c7e64c7996bb9d6\n",
      "https://ie.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0BI8IerbVtBNz9lxt-gGwgL1yqzg7rkL65oBt9kXiGchRfMu5HD70ENCeZDeMdLuF3Q6xzcJEOdJs3kZF1vPa7hsSvWDw387HGAhy09ybLyjuHgMbirifct4XvSysIpp9rS6Ba6AU4LkoNkAgvufXadgt9JcsvxlhJuDKpnvDOFeBo5dzQWo7pFgkc2ydoCMEtW2iWHYK8s0NBZGIKl4r7fRa5BSkndStcyPVvOs74vHKeTHQ7SUXy9Jvocb78gEg_dj47DKiNE4DkFJO0_gAKH8J7zdVJ2_SA0SWcKExPMVuj8cvElgzB1sNgtlr6Tj_tvJYbJguf9j_ZbW40QusqIW71ACZtcIEQtNDB2ACq92fxa2jeykoUuFHgXBCVybxWvDPNxEKC8ZR-CQ2U-uxIe24BspMOf_Q7ubq8y7Rn2TC_ww1c3qeBtg0-HG1nayqXJlu8_Q8JQxpi2Orw0JMwSvDbknG-Palp8oXVzt3wQSEGXv7POm22gus8PSdFTRhqn53K7WFof92B7UKnzoXDCOCUog7ovhXz-0jUTkR7jGF9BISOe6IYI7eSukOK5IeKuwIwAbEZoHgZeUmjLEFn634S8b_7FwUAq9oF0v_2kip74rvbHu1veWC9xeqPIVLR7LE43bTRiSpyNt1sryfSapz_GPLdBSSY=&xkcb=SoAG6_M3G50qpcR9Np0LbzkdCdPP&p=0&fvj=1&vjs=3\n",
      "Posted\n",
      "Just posted\n",
      "JOB AD NO.2:\n",
      "Clinical Nurse Manager (CNM)\n",
      "sj_358f1f68cde928c4\n",
      "https://ie.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0AVCuTJkMSq0gqIpm6bhV7v6tYBRH9u_MNnmp83dQl-I38w5fsjm0KOQH0nDZjSOcOPYeWFJgadE8QI-f8AsCzthoXfTX1WmFazIZi6tjV2ing9wXyxXhp7lrHTEwHGw33TyV5MyVmDISWer-boO4hTAqMhmFiAtf7IDTnpI23K40Jij9Q0j6aW2aOWk820si5mPPWVhm7AIkSwW8dHZlrys2oP-ctiwHu5cwgK7ciV5OPb7h0mpWne8TXN-KHFWIZln_ehdQkSNO2_tZ7aoHStdDhELSKkzN7n0C3WtaW--k8LwviHm8BaMv07tLVWjGGLUNQ9hR_-tZ8imUhykD44K4RQ-ZndbqRvovo6zKTRGs7iY8lYX1UXgOFQLyPzJ5hhE0gfRllgdkKsd3IbKwd4HnS7gi9ZL55Awuh7XRmQPb_fYZPNJ-OvuflX2z22DL_EGrTcnFNl5VA-ZR52eEnqq9JktL8cmhMQbsCpLROzSsF2eS9A5ILOoEBR_A2ECD3mGLo-FvcVgx9-bBVyG9Ch-GmlSFUg4PALHicwMkW4_7nHUVblakfQAD0znGLIefY9CGJYbHyxSKOzhkyHYLW6jIqcjUmLBV98LHmVdNU0hU_Dtuq5BbXVtY1pmcRPQ1qxzSDuPIpO9FkhSUFnTLpY-QfpAirJSoQ=&xkcb=SoCy6_M3G50qpcR9Np0KbzkdCdPP&p=1&fvj=1&vjs=3\n",
      "Hiring ongoing\n",
      "\n",
      "\n",
      "The extraction was completed in: 5.0 minutes and 22.88646674156189 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Scrape the data available on Indeed.com and save into 'job_list' variable.\n",
    "scrape_job_details(jobs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The raw data was transformed and exported successfully as C:\\Users\\temulenbd\\OneDrive\\Desktop\\learn\\github_repo\\cct\\capstone_project\\data_jobads_rn.csv.\n"
     ]
    }
   ],
   "source": [
    "# Transform the scraped data and export it as a CSV file.\n",
    "df_create_export_csv(df_name[0], csv_file_name[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset variables.\n",
    "max_iter_pgs = int()\n",
    "job_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*electrician ads*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ELECTRICIAN\n",
      "Total number of vacancies available in Dublin area on January 10, 2024: 166 jobs.\n",
      "Maximum number of iterable pages for the search: 11 pages\n",
      "\n",
      "\n",
      "Action was completed in: 15.139153242111206 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Check the job availability on Indeed.com.\n",
    "get_job_info(jobs[1], job_titles[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ELECTRICIAN JOB ADS\n",
      "The total count of scraped job vacancies: 155 jobs.\n",
      "\n",
      "JOB AD NO.1:\n",
      "Industrial Electrician\n",
      "sj_7cba7a465e6641fe\n",
      "https://ie.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0AnEYSss4eDbTLySY2p2efwzP3CilC5Xfeyf166GCcN9JJMj_EWM2A3xodspN8Pi50AQZK4e0X589pQszonXkMkH0RQ88koQVn7ZT1ssPBpQ_NkkqQhHLy474KArTvD5gBqTeIgUZnCJu_dKf5oe4HAkKcLEP2oyZYldxw_5f6gvn14xcRPMRza8-QyIqzv0byDd4PvDXXzJE4qOMdLC5mFAHmK1MYBMw5p7DiJwb-0l9Wn2nPXXR0fXfDw3ZGgpTK2fEod5hefk8SHbbBTg5XXqH-cqkCUwZQ9x2EWDy0u4UzQIublT1xvmjgu6FKChkkSwYiOeLdBpClFaBomH_21iQ0g5LHJAjhpQKbe5Rsk1qWO2CAoxB8uzG0hGry5nE_Kk10xn7b1s6IhQqZOo55YeLj-GzbQ2w-DTtb3Sp4X3C8eZfASRrQDZn5dl8gS8f63CC-Vla0OSTSqrKEexz5CHQ9huC6i-HpqLL5cp0JRmXoTInszOBMVKdAONBilh2S8z-AKr5sldSdsgOg8u-rA4qQggC-Z_S6yGMKpGlEqi3SmevB2y9AV74UJrhYkGLBICcE3LTwSgJcgH953pBiTYB-Ykz2oCw3HwOcSIklmmHLJtX1pEGpD9CH-Kc449-wE79tKz3lqJA==&xkcb=SoBz6_M3G51AfAR9Np0LbzkdCdPP&p=0&fvj=1&vjs=3\n",
      "Posted\n",
      "Just posted\n",
      "JOB AD NO.2:\n",
      "Apprentice Electrician\n",
      "job_a6937c5385359d51\n",
      "https://ie.indeed.com/rc/clk?jk=a6937c5385359d51&bb=j7vMUYt2-bi_4MmLfG7BPwbndCrrWJzJhXCu32Q571yx9H_OeNsgAGawfb_SkW2e4AVoT8Q2rWmlax6AWUD7itP5hv9nZ_uvuhRkI_-Y07E%3D&xkcb=SoAz67M3G51AfAR9Np0KbzkdCdPP&fccid=c021a655a4b9e800&cmp=STS-Group&ti=Apprentice+Electrician&vjs=3\n",
      "Posted\n",
      "Just posted\n",
      "\n",
      "\n",
      "The extraction was completed in: 1.0 minutes and 43.00874090194702 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Scrape the data available on Indeed.com and save into 'job_list' variable.\n",
    "scrape_job_details(jobs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The raw data was transformed and exported successfully as C:\\Users\\temulenbd\\OneDrive\\Desktop\\learn\\github_repo\\cct\\capstone_project\\data_jobads_e.csv.\n"
     ]
    }
   ],
   "source": [
    "# Transform the scraped data and export it as a CSV file.\n",
    "df_create_export_csv(df_name[1], csv_file_name[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset variables.\n",
    "max_iter_pgs = int()\n",
    "job_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*data analyst ads*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA ANALYST\n",
      "Total number of vacancies available in Dublin area on January 10, 2024: 342 jobs.\n",
      "Maximum number of iterable pages for the search: 22 pages\n",
      "\n",
      "\n",
      "Action was completed in: 12.003286123275757 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Check the job availability on Indeed.com.\n",
    "get_job_info(jobs[2], job_titles[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA+ANALYST JOB ADS\n",
      "The total count of scraped job vacancies: 315 jobs.\n",
      "\n",
      "JOB AD NO.1:\n",
      "Inventory Analyst\n",
      "sj_d72c4cf42121ca22\n",
      "https://ie.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0AnxfxssoujI3GvYsxvmNCzxzm6VnPtwq4vmbotio0JvQP4qXO3kiNsl1yUmVNF84MzBzmk8-uNcalZeVGMR2tlbw0j0ghDB32zhGcWEZiFsh6Zhjvcky-uhasSq90-Q5tQuwoeDbw8SltNynq_LNJvZUMRCpbvjFC48w4dMABSo4ebojJMmZ--dHZHBiOPE135_T6Xa7ud4X-cfbHmQ6gOKJv1AWhNFU4c5f5OSZMb91QEb925KxbCkocbzbpn9dQx3uhvO9W8KGpcJhKtOfLETuEJsfe-UuHiydeNE0qvY3PL0wvF7dhZ_XawpnVmj6C9DhBC5cnxriFAefUXHrWNQmj-WT9vVpoL95aOJdIc7s7y1ypjJI3yNWLyvdOVs6umuVRUmwN_uqs6ePa9Dc4h8SHkPM_GPMXsZLziJ3wN7iIsmpT6cHqk2BA4lWfDKw0S5d6TckE8Xzv-8UyHiDSQaOsejKEjKLi1uYZCRQAnHUSP5nZLrZdqQ18ZggJQSwBOJNIXuo3CsOYtW0UYGcMiBfWpdPuCYCdmtzEAcKghJZr_S1MYHgV56H-H6wpXyZ44S6SmdX4gn64qpFPnTP6sWpx5E6Pd6TpjCYvf9GoRuaeJDPvU4xDHpCQpEE2ih4frPZwuYx3gDPR5XMbklqnoNQaAOZZ4X18oOwIhhmHa4kVpQor0_f9mKTUHw5oxri0AkpEu-qcHoQ==&xkcb=SoDk6_M3G51RjlQHEx0LbzkdCdPP&p=0&fvj=0&vjs=3\n",
      "Posted\n",
      "Just posted\n",
      "JOB AD NO.2:\n",
      "Defect Control Engineer with Strong data analysis and programing skills (SQL/Python/MATLAB).\n",
      "job_9d518bcdd2dc8ca0\n",
      "https://ie.indeed.com/rc/clk?jk=9d518bcdd2dc8ca0&bb=WFqqXi9ONJgXrTWGjxsH4_o-jPUX29-mPu0LCwUqhMc0TNGLxpap4cREZ8pq4yGwglpravmc3kDvRNT5iTj29WA89UGPwrWw-ZQBSBmS-ss%3D&xkcb=SoCk67M3G51RjlQHEx0KbzkdCdPP&fccid=936367796261bd6e&vjs=3\n",
      "Posted\n",
      "Just posted\n",
      "\n",
      "\n",
      "The extraction was completed in: 2.0 minutes and 59.2672963142395 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Scrape the data available on Indeed.com and save into 'job_list' variable.\n",
    "scrape_job_details(jobs[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The raw data was transformed and exported successfully as C:\\Users\\temulenbd\\OneDrive\\Desktop\\learn\\github_repo\\cct\\capstone_project\\data_jobads_da.csv.\n"
     ]
    }
   ],
   "source": [
    "# Transform the scraped data and export it as a CSV file.\n",
    "df_create_export_csv(df_name[2], csv_file_name[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 20th JAN, 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define global variables.\n",
    "jobs = ['registered+nurse', 'electrician', 'data+analyst']\n",
    "job_titles = ['registered nurse', 'electrician', 'data analyst']\n",
    "job_list = []\n",
    "pagination_url = 'https://ie.indeed.com/jobs?q={}&l=Dublin%2C+County+Dublin&radius=25&filter=0&sort=date&start={}'\n",
    "max_iter_pgs = int()\n",
    "directory = r'C:\\Users\\temulenbd\\OneDrive\\Desktop\\learn\\github_repo\\cct\\capstone_project'\n",
    "df_name = ['df_rn', 'df_e', 'df_da']\n",
    "csv_file_name = ['data_jobads_rn_20jan.csv', 'data_jobads_e_20jan.csv', 'data_jobads_da_20jan.csv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom function that verifies how many positions are available for the specified job and how many pages can be iterated.\n",
    "def get_job_info(job_to_look, job_print):\n",
    "    \n",
    "    # Declare global variables.\n",
    "    global pagination_url\n",
    "    global max_iter_pgs\n",
    "    job = job_to_look\n",
    "    \n",
    "    # Set up Chrome webdriver options.\n",
    "    option= webdriver.ChromeOptions()\n",
    "    option.add_argument(\"--incognito\")\n",
    "    \n",
    "    # Specify the date.\n",
    "    current_date = datetime.now().date().strftime('%B %d, %Y')\n",
    "    start = time.time()\n",
    "    \n",
    "    # Initialize Chrome driver\n",
    "    driver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()), options=option)\n",
    "    driver.get(pagination_url.format(job, 0))\n",
    "\n",
    "    sleep(randint(4, 9))\n",
    "    job_number = driver.find_element(By.CLASS_NAME,'jobsearch-JobCountAndSortPane-jobCount').text\n",
    "    max_iter_pgs=int(job_number.split(' ')[0]) // 15 \n",
    "\n",
    "    # Close the WebDriver.\n",
    "    driver.quit()\n",
    "    end = time.time()\n",
    "\n",
    "    # Print results.\n",
    "    print(f'{job_print.upper()}')\n",
    "    print(f'Total number of vacancies available in Dublin area on {current_date}: {job_number}.')\n",
    "    print('Maximum number of iterable pages for the search:', max_iter_pgs, 'pages')\n",
    "    print('\\n')\n",
    "    print('Action was completed in:', end - start, 'seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a costum function that will extract data from the web-page and create a table with the information for the specified job.\n",
    "def scrape_job_details(job_to_look):\n",
    "    \n",
    "    # Declare global variables.\n",
    "    global max_iter_pgs\n",
    "    global job_list\n",
    "    global job_titles\n",
    "    global pagination_url\n",
    "    job =  job_to_look\n",
    "    \n",
    "    # Specify the date.\n",
    "    start = time.time()\n",
    "    \n",
    "    # Set up Chrome WebDriver.\n",
    "    driver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()))\n",
    "    sleep(randint(4, 9))\n",
    "\n",
    "    # Loop through each job posting through the pages and extract job details.\n",
    "    for i in range(0, max_iter_pgs):\n",
    "        driver.get(pagination_url.format(job, i * 10))\n",
    "        sleep(randint(4, 9))\n",
    "\n",
    "        job_page = driver.find_element(By.ID, 'mosaic-jobResults')\n",
    "        job_posts = job_page.find_elements(By.CLASS_NAME, 'job_seen_beacon')\n",
    "\n",
    "        for job_post in job_posts:\n",
    "            job_title = job_post.find_element(By.CLASS_NAME, 'jobTitle')\n",
    "            job_id = job_title.find_element(By.CSS_SELECTOR, 'a').get_attribute(\"id\")\n",
    "            job_link = job_title.find_element(By.CSS_SELECTOR, 'a').get_attribute(\"href\")\n",
    "\n",
    "            try:\n",
    "                job_date = job_post.find_element(By.CLASS_NAME, 'date').text\n",
    "            except Exception as e:\n",
    "                job_date = 'not available'\n",
    "\n",
    "            # Append job details to the job_list.\n",
    "            job_list.append([job_title.text, job_id, job_link, job_date])\n",
    "    \n",
    "    # Close the WebDriver.\n",
    "    driver.quit()\n",
    "    end = time.time()\n",
    "\n",
    "    # Check results.\n",
    "    print(f'{job.upper()} JOB ADS')\n",
    "    print(f'The total count of scraped job vacancies: {len(job_list)} jobs.\\n')\n",
    "    for x in range(min(2, len(job_list))):\n",
    "        print(f'JOB AD NO.{x + 1}:')\n",
    "        print(job_list[x][0])\n",
    "        print(job_list[x][1])\n",
    "        print(job_list[x][2])\n",
    "        print(job_list[x][3])\n",
    "    print('\\n')\n",
    "\n",
    "    print(f'The extraction was completed in: {(end - start) // 60} minutes and {(end - start) % 60} seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a costum function that creates a new DataFrame of given job title, transforms the data, exports it as a CSV file.\n",
    "def df_create_export_csv(new_df, csv):\n",
    "    \n",
    "    # Declare global variables.\n",
    "    global job_list\n",
    "    global directory\n",
    "    \n",
    "    # Create a new pandas Dataframe with the given job title.\n",
    "    column = ['title', 'id', 'link', 'date']\n",
    "    new_df = pd.DataFrame(job_list, columns=column)\n",
    "\n",
    "    # Loop through each row in the DataFrame for data transformation.\n",
    "    for x in range(int(new_df.shape[0])):\n",
    "        new_df.iat[x, 0] = new_df.iat[x, 0].lower()\n",
    "        new_df.iat[x, 3] = new_df.iat[x, 3].replace('Posted\\n', '')\n",
    "        new_df.iat[x, 0] = new_df.iat[x, 0].lower()\n",
    "\n",
    "    # Create the file path for CSV export.\n",
    "    file_path = os.path.join(directory, csv)\n",
    "    \n",
    "    # Export the DataFrame to CSV file.\n",
    "    # new_df.to_csv(file_path, index=False)\n",
    "    \n",
    "    print(f\"The raw data was transformed and exported successfully as {file_path}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Web scraping\n",
    "**registered nurse ads**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REGISTERED NURSE\n",
      "Total number of vacancies available in Dublin area on January 20, 2024: 625 jobs.\n",
      "Maximum number of iterable pages for the search: 41 pages\n",
      "\n",
      "\n",
      "Action was completed in: 14.182044982910156 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Check the job availability on Indeed.com.\n",
    "get_job_info(jobs[0], job_titles[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REGISTERED+NURSE JOB ADS\n",
      "The total count of scraped job vacancies: 612 jobs.\n",
      "\n",
      "JOB AD NO.1:\n",
      "Cardiac Staff Nurse\n",
      "job_1df6cdf12a7ff3b4\n",
      "https://ie.indeed.com/rc/clk?jk=1df6cdf12a7ff3b4&bb=KDvhOqIgqZ5NSFT5QjrOO23C8PptS_z7nVKCwpR56LoW4Fzx2imYf-7VMaZlwzzxoISVquSka--EG3V8ehIRKA5OjDsy4MsGmD2jxDyABCk%3D&xkcb=SoBL67M3FTSZ_k07Kh0LbzkdCdPP&fccid=c6715a18e860f1f6&cmp=White-Label-Management&ti=Cardiac+Nurse&vjs=3\n",
      "Posted\n",
      "Today\n",
      "JOB AD NO.2:\n",
      "Theatre Staff Nurse\n",
      "job_8dac085957f00f90\n",
      "https://ie.indeed.com/rc/clk?jk=8dac085957f00f90&bb=KDvhOqIgqZ5NSFT5QjrOO6vLP6RzbBCm65ZCHQdoTmzEiqePkoPCU2j6-hqwGtfBBeRpxgNS3TG6UM9DKg08LpgcfZy1hPi48cPuJnbG5Go%3D&xkcb=SoD_67M3FTSZ_k07Kh0KbzkdCdPP&fccid=c6715a18e860f1f6&cmp=White-Label-Management&ti=Theatre+Nurse&vjs=3\n",
      "Posted\n",
      "Today\n",
      "\n",
      "\n",
      "The extraction was completed in: 5.0 minutes and 17.33529567718506 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Scrape the data available on Indeed.com and save into 'job_list' variable.\n",
    "scrape_job_details(jobs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The raw data was transformed and exported successfully as C:\\Users\\temulenbd\\OneDrive\\Desktop\\learn\\github_repo\\cct\\capstone_project\\data_jobads_rn_20jan.csv.\n"
     ]
    }
   ],
   "source": [
    "# Transform the scraped data and export it as a CSV file.\n",
    "df_create_export_csv(df_name[0], csv_file_name[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset variables.\n",
    "max_iter_pgs = int()\n",
    "job_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**electrician ads**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ELECTRICIAN\n",
      "Total number of vacancies available in Dublin area on January 20, 2024: 151 jobs.\n",
      "Maximum number of iterable pages for the search: 10 pages\n",
      "\n",
      "\n",
      "Action was completed in: 16.498794078826904 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Check the job availability on Indeed.com.\n",
    "get_job_info(jobs[1], job_titles[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ELECTRICIAN JOB ADS\n",
      "The total count of scraped job vacancies: 145 jobs.\n",
      "\n",
      "JOB AD NO.1:\n",
      "Maintenance Technician (Electrical) - Dublin 3 (AM18158)\n",
      "sj_8406b1af34f03d2b\n",
      "https://ie.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0D8963UVSHQvE14Gz87xEjFAazKwo8pwGt2AcCk0nHr42kDSuf3Mi8ZyaVg53Qijir90_Qm1Q588QQ6lWUwXDJ9nXC3sedbrJN4vxMujoOrQL0P5BIeuVMeMEdWqKYoybDsEv-NLXMbDBTSK3axGohD5OOQrNi0BrvJrdlam0i1_kdtCLwFtT0UwVETkTSJlFdYUqkYowxlhmhAAj-DGUAn7ppL5nftu46BXs_hX00-9myD5T9LcnIx0xATWT8kBCkAa9MKK5MVU73xK5joXIIrlK4_owpuEg6_dglcR6u1G9NK9ZcNoXIjFV_vw7Uo4_IHMJiqhZYm7LMtoUx3ufx8oQy9ThJmsA7VBhClDeIpp8xhycYf4v5k-EJpZMRRqatfYIwToH6y777i3bX6gAwoMwpv7xgE-c_FxeGCt2b2oLnnll1S7XHF09wGR4NK3vwGgitHCOBZ_gbYOjei-V6sAz3rlads2XRQM5oZm7RxVje364Sk_9DZGjkx2w3APZDc_9giWsD5eLo_v6HYUzj24SuUMZpbjnAcJQ1t0JNstpTSIUm4z1Ua68G8dxFp6KYBWx0eThN_BgxtUGIJq4WfUmYQmdk0yOsnpwLehOHXJeYIK5CTNfixZKZy3a8wFJJLnsAjCnq-YCMcIMhT6q50dxVjdvBwjvrSlso9yXZDN3bd1HRBIk0Z&xkcb=SoC86_M3FTSwrK07Mx0LbzkdCdPP&p=0&fvj=1&vjs=3\n",
      "Posted\n",
      "Today\n",
      "JOB AD NO.2:\n",
      "Electrician\n",
      "sj_90090379dc44bbc5\n",
      "https://ie.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0BAkp0XFzye14tYByJ9bG7_G8t2H0RsHXoy06gAIxoqHWudm3mQ_GjHMQrQzWg9cjU1crEor7oYuzvKPNPvmQmUuv2HBodb45Kud5RTh-pPT6lmKekqLMKKhywhGj7ELfGmlX7XKlDdhMLSCbY9GqT0lqNrBPBsfxxrUg1mrjq1Bxiv19mpk3gVcd3rqNsJMRVWD4KbYh_MSrDhFI_DpzQD35whKdRrPY3nK-ixVnT9m-NpaVivaXF3SZ4atPvTq0XiOukJMGwEiScm4IqWluaR9om-b9Ys3Ok-Swm0GJxqKwegtchzkhVmVPenOgzuN_HWQnMZ_GEVkx7iifo4eejrPGHxIaFPCcyT44wgypSQ5vtCS3kmxwsSefzmkEX7dFwygZAmZRzrDEBD9J3-53ciED24iVwMBR_RDiXKCEqZfzYBDCIT6w8bTDvjhFcFjTyfNTxmWP0A5K7R7wGiw-QX2tSZoAz2z4We6pkPsTWXuaSQja-veZNIefNyVDduNeozjcPazAYilewUdKYx3F1fq_4bpUyM4mFK83wMDatVSIqQBgVXjx1Pt520Rd5YQg2KoqVe8yqoK5tCS7NBSNkyqbRryj_0dAR18igLToOI-3APqArignoxU0S2PfiU4Lw=&xkcb=SoAI6_M3FTSwrK07Mx0KbzkdCdPP&p=1&fvj=1&vjs=3\n",
      "Employer\n",
      "Active 2 days ago\n",
      "\n",
      "\n",
      "The extraction was completed in: 1.0 minutes and 29.19952654838562 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Scrape the data available on Indeed.com and save into 'job_list' variable.\n",
    "scrape_job_details(jobs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The raw data was transformed and exported successfully as C:\\Users\\temulenbd\\OneDrive\\Desktop\\learn\\github_repo\\cct\\capstone_project\\data_jobads_e_20jan.csv.\n"
     ]
    }
   ],
   "source": [
    "# Transform the scraped data and export it as a CSV file.\n",
    "df_create_export_csv(df_name[1], csv_file_name[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset variables.\n",
    "max_iter_pgs = int()\n",
    "job_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**data analyst ads**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA ANALYST\n",
      "Total number of vacancies available in Dublin area on January 20, 2024: 346 jobs.\n",
      "Maximum number of iterable pages for the search: 23 pages\n",
      "\n",
      "\n",
      "Action was completed in: 17.141117811203003 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Check the job availability on Indeed.com.\n",
    "get_job_info(jobs[2], job_titles[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA+ANALYST JOB ADS\n",
      "The total count of scraped job vacancies: 334 jobs.\n",
      "\n",
      "JOB AD NO.1:\n",
      "Data Analyst\n",
      "job_3ffe8294304602bd\n",
      "https://ie.indeed.com/rc/clk?jk=3ffe8294304602bd&bb=Lda2kDTxpK_Bw6ZiyREAuz9Bque7xHJNVLFtjAnziN5Rikp1PLpDzD63vAnfHjd2GGsoOHslDLTNc1jHq4XAFOv-nHBFjj1w299F_1-gEsA%3D&xkcb=SoC367M3FTTCt1WHMZ0LbzkdCdPP&fccid=e74773ca4b4eccf9&vjs=3\n",
      "Posted\n",
      "Just posted\n",
      "JOB AD NO.2:\n",
      "Data Analyst\n",
      "job_84870afff879430a\n",
      "https://ie.indeed.com/rc/clk?jk=84870afff879430a&bb=Lda2kDTxpK_Bw6ZiyREAu1JmhJayD93Zna4KCNZPIeMyOckg4Cld4HDuA1FJ7Bl4ftI0c8POrW2ax59bVi2U8_kCNm04f0fL7UBE3IYXDWg%3D&xkcb=SoAD67M3FTTCt1WHMZ0KbzkdCdPP&fccid=855f48b961808012&vjs=3\n",
      "Posted\n",
      "Today\n",
      "\n",
      "\n",
      "The extraction was completed in: 3.0 minutes and 3.578531503677368 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Scrape the data available on Indeed.com and save into 'job_list' variable.\n",
    "scrape_job_details(jobs[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The raw data was transformed and exported successfully as C:\\Users\\temulenbd\\OneDrive\\Desktop\\learn\\github_repo\\cct\\capstone_project\\data_jobads_da_20jan.csv.\n"
     ]
    }
   ],
   "source": [
    "# Transform the scraped data and export it as a CSV file.\n",
    "df_create_export_csv(df_name[2], csv_file_name[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I.II Job offers' data collection (extraction of the job description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10th JAN, 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Setting up for web scraping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define global variables.\n",
    "job_details = []\n",
    "csv_files = ['data_jobads_rn.csv', 'data_jobads_e.csv', 'data_jobads_da.csv']\n",
    "directory = r'C:\\Users\\temulenbd\\OneDrive\\Desktop\\learn\\github_repo\\cct\\capstone_project'\n",
    "df_name = ['df_rn', 'df_e', 'df_da']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom function to extract job details from the available hyperlinks.\n",
    "def get_job_details(csv):\n",
    "    \n",
    "    # Declare global variables.\n",
    "    global job_details\n",
    "    \n",
    "    # Set up Chrome webdriver options.\n",
    "    option=Options()\n",
    "    option.add_experimental_option('debuggerAddress', 'localhost:0820')\n",
    "    \n",
    "    # Specify the start time.\n",
    "    start = time.time()\n",
    "    \n",
    "    # Initialize Chrome driver.\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=option)\n",
    "    \n",
    "    # Access each hyperlink, retrieve information about the job, and store it in the global variable 'job_details'.\n",
    "    df_ads = pd.read_csv(csv, index_col=None)\n",
    "    total_rows = df_ads.shape[0]\n",
    "    \n",
    "    for x in range(0, total_rows):\n",
    "        link = df_ads.iat[x, 2]\n",
    "        driver.get(link)\n",
    "        sleep(randint(2, 4))\n",
    "    \n",
    "        job_page = driver.find_element(By.ID, 'jobDescriptionText')\n",
    "        job_details.append(job_page.text)\n",
    "        sleep(randint(2, 4))\n",
    "        \n",
    "    # Specify the end time.\n",
    "    end = time.time()\n",
    "    \n",
    "    # Check results.\n",
    "    print(f'Total number of extracted data: {len(job_details)}.\\n')\n",
    "    print('EXAMPLE:')\n",
    "    print(job_details[randint(0, total_rows)], '\\n')\n",
    "    print(f'The extraction was completed in: {(end - start) // 60} minutes and {(end - start) % 60} seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a costum function to rewrite extracted information into existing csv files.\n",
    "def df_create_export_csv(new_df, csv):\n",
    "\n",
    "    # Declare global variables.\n",
    "    global job_details\n",
    "    global directory\n",
    "    \n",
    "    # Create a new pandas Dataframe using the ads csv file.\n",
    "    new_df = pd.read_csv(csv, index_col=None)\n",
    "    new_df['job_description'] = job_details\n",
    "    \n",
    "    # Create the file path for CSV export.\n",
    "    file_path = os.path.join(directory, csv)\n",
    "    \n",
    "    # Export the DataFrame to CSV file.\n",
    "    # new_df.to_csv(file_path, index=False)\n",
    "    \n",
    "    print(f\"The raw data was  rewritten to existing file and successfully exported as {file_path}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Web scraping\n",
    "**registered nurse ads**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of extracted data: 564.\n",
      "\n",
      "EXAMPLE:\n",
      "We are now hiring Nurses in Dublin\n",
      "Allied and Clinical are now Recruiting Nurses (all grades welcome) for Agency shifts in Tallaght Hospital.\n",
      "Excellent opportunity for Nurses looking for a better work life balance, career change or extra shifts.\n",
      "At Allied and Clinical, our healthcare professionals come first. We strive to treat each applicant as a person with their own career path and ambitions by being professional, warm, and approachable.\n",
      "Why Choose Allied and Clinical?\n",
      "-Choose your own working schedule.\n",
      "-Competitive rates of pay in line with pay scales (€16.67-€57.00 per hour)\n",
      "-Choice of different healthcare facilities.\n",
      "-Gain experience in private/public healthcare facilities.\n",
      "-One to One consultancy.\n",
      "-Refer a friend bonus scheme\n",
      "-Free Uniform.\n",
      "-Free mandatory training.\n",
      "-Free Life support training.\n",
      "-Free Fit to work\n",
      "Schedule:\n",
      "Day Shift\n",
      "Night Shift\n",
      "12 HR Shift\n",
      "8 HR Shift\n",
      "Experience\n",
      "-Registered on the Irish live register (NMBI). Must have active PIN.\n",
      "-Strong Listening and communication skills.\n",
      "-Compassionate and empathetic, experience in providing person centred care\n",
      "Salary: €33,943-€53,280 per year\n",
      "Job Types: Full-time, Part-time\n",
      "Salary: €33,943.00-€50,135.00 per year\n",
      "Benefits:\n",
      "Flexitime\n",
      "Schedule:\n",
      "Day shift\n",
      "Monday to Friday\n",
      "Night shift\n",
      "Weekend availability\n",
      "Licence/Certification:\n",
      "Nursing and Midwifery Board of Ireland registration (required)\n",
      "Work Location: In person \n",
      "\n",
      "The extraction was completed in: 61.0 minutes and 36.95956468582153 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Scrape the job details.\n",
    "get_job_details(csv_files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The raw data was  rewritten to existing file and successfully exported as C:\\Users\\temulenbd\\OneDrive\\Desktop\\learn\\github_repo\\cct\\capstone_project\\data_jobads_rn.csv.\n"
     ]
    }
   ],
   "source": [
    "# Update the extracted data and save the changes.\n",
    "df_create_export_csv(df_name[0], csv_files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset variable.\n",
    "job_details = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**electrician ads**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of extracted data: 155.\n",
      "\n",
      "EXAMPLE:\n",
      "General information\n",
      "Organisation\n",
      "Egis is an international player active in the consulting, construction engineering and mobility service sectors.\n",
      "We design and operate intelligent infrastructure and buildings capable of responding to the climate emergency and helping to achieve more balanced, sustainable and resilient territorial development.\n",
      "With operations in 120 countries, Egis places the expertise of its 18,000 employees at the disposal of its clients and develops cutting-edge innovation accessible to all projects.\n",
      "Improving people's quality of life and supporting communities in their social and economic development, whilst drastically reducing carbon emissions and achieving vital 2050 net zero targets, that's our purpose.\n",
      "Reference\n",
      "2023-9904\n",
      "Position description\n",
      "Job title\n",
      "Maintenance Technician M/F\n",
      "Contract type\n",
      "Permanent contract\n",
      "Business Line specific context\n",
      "POSITION:\n",
      "Maintenance Technician\n",
      "\n",
      "COMPANY:\n",
      "Egis Road & Tunnel Operation\n",
      "\n",
      "PROJECT:\n",
      "Egis Road and Tunnel Operation Ireland (ERTO) is an Irish company established in 2005 and is part of Egis Projects Ireland. We manage Dublin Tunnel, Jack Lynch Tunnel in Cork and the Motorway Traffic Control Centre (MTCC) for Transport Infrastructure Ireland (TII).\n",
      "ERTO's responsibility is to manage all processes associated with the Operation and Maintenance of the Dublin Tunnel, Jack Lynch Tunnel and the MTCC. This includes Toll Collection, Traffic Management, Tunnel Safety Management, Emergency and Contingency Planning, Equipment and Infrastructure Inspection and Maintenance. We are located in the Dublin Tunnel Control Building on East Wall Road, Dublin 3.\n",
      "Job description\n",
      "This role is responsible for all mechanical and electrical systems throughout the Dublin Tunnel affected property. Reporting to the M&E Technical Manager. The role will also involve aspects of civil engineering as required.\n",
      "\n",
      "RESPONSIBILITIES & MAIN ACTIVITIES, INCLUDING BUT NOT LIMITED TO:\n",
      "\n",
      "Maintenance of predominately Electrical and Mechanical Systems.\n",
      "Manage records and documentation of all maintenance activities in the Maintenance Management System.\n",
      "Ensure day to day preventive and corrective maintenance work orders are completed in a timely and cost effective manner.\n",
      "Attendance, supervision and management of scheduled & other tunnel closures.\n",
      "Flexibility to attend and manage contractors during maintenance operations including tunnel closure works.\n",
      "Assist in the implementation of equipment and system upgrade projects (CAPEX).\n",
      "Manage and support projects to improve equipment reliability, performance and availability.\n",
      "Develop, manage and supervise projects undertaken by the Maintenance Department.\n",
      "Provide on-call cover for out of hours Maintenance activities undertaken as part of a roster.\n",
      "System owner of complete systems e.g. ventilation, MV, LV, power etc, inclusive of the management of related Contractors/Suppliers.\n",
      "MV experience is desirable.\n",
      "Prepare and review Method Statement Risk Assessments (MSRA).\n",
      "Assistance during accidents/incidents inclusive of witness statements and report writing.\n",
      "Assistance during recovery of vehicles inclusive of report writing.\n",
      "Civil/structural/environmental inspections as deemed necessary.\n",
      "\n",
      "Profile\n",
      "QUALIFICATIONS, SKILLS & EXPERIENCE:\n",
      "3 years’ experience as a maintenance technician\n",
      "Fully qualified electrician is an essential criteria for the role.\n",
      "Computer literacy – Microsoft Word / Excel / Outlook in particular - please be aware that this role is 50% hands on and 50% desk based.\n",
      "Full clean drivers license and own transport.\n",
      "Must live within 45 minutes driving distance from ERTO when on call.\n",
      "Position location\n",
      "Job location\n",
      "Europe, Ireland\n",
      "City\n",
      "Dublin\n",
      "Candidate criteria\n",
      "Minimum level of education required\n",
      "3-Diploma of Higher Education / Associate’s Degree / BTEC Higher National Diploma\n",
      "Minimum level of experience required\n",
      "2-5 years\n",
      "Languages\n",
      "English (5- Proficiency) \n",
      "\n",
      "The extraction was completed in: 16.0 minutes and 17.172507762908936 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Scrape the job details.\n",
    "get_job_details(csv_files[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The raw data was  rewritten to existing file and successfully exported as C:\\Users\\temulenbd\\OneDrive\\Desktop\\learn\\github_repo\\cct\\capstone_project\\data_jobads_e.csv.\n"
     ]
    }
   ],
   "source": [
    "# Update the extracted data and save the changes.\n",
    "df_create_export_csv(df_name[1], csv_files[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset variable.\n",
    "job_details = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**data analyst ads**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of extracted data: 315.\n",
      "\n",
      "EXAMPLE:\n",
      "As a member of the Accounting Operations team, you will be responsible for assisting day-to-day operations and continuous improvement initiatives for the Accounting department. Your main tasks will include gathering data, performing analysis, and supporting projects and initiatives as needed.\n",
      "\n",
      "A typical day might include the following:\n",
      "Perform ad hoc requests related to reporting and data analysis to assist other team members, management, and audit requests.\n",
      "Support the analysis and implementation of accounting operation functions relating to new technology projects throughout the organization.\n",
      "Ensure quality control over the financial transactions and financial reporting.\n",
      "Demonstrating knowledge of technical accounting standards under IFRS and US GAAP, and its application to tasks at hand.\n",
      "Support month-end and year-end close processes.\n",
      "Prepare documentation and Standard Operating Procedures (SOPs) for processes, enhancements and projects as needed.\n",
      "Responsible for preparation of various schedules impacting the month-end close and reporting.\n",
      "Posting of accruals, prepayments any other adjustments in order to complete monthly management accounts within deadlines.\n",
      "Assist team with the execution of test scripts for various projects.\n",
      "Participating in process improvement initiatives\n",
      "Support the development of key internal controls and procedures, and ensuring internal controls are designed and operating effectively in compliance with the provisions of the Sarbanes-Oxley Act\n",
      "Demonstrating excellent verbal and written communication skills, including effective presentation skills\n",
      "High proficiency with Microsoft applications is required.\n",
      "\n",
      "This role might be for you if:\n",
      "You have worked in an accounting function for 3 - 5 years and/or hold a BS/BA in Accounting or are recently qualified ACA/ACCA\n",
      "You are proactive and analytical problem solver with the ability to think outside of the box.\n",
      "You have the willingness to learn new processes and technologies and can work in a collaborative and fast paced environment.\n",
      "You can communicate effectively with colleagues and peers both verbally and non-verbally.\n",
      "You are coachable, committed to change, and open to embrace new ideas and perspectives.\n",
      "You are dependable, have strong organizational abilities, pay attention to details, and most importantly enjoy your work and have fun while doing it.\n",
      "Experience with ERP financial applications, Oracle applications a distinct advantage.\n",
      "High proficiency with Microsoft applications, Advanced Microsoft Excel knowledge a necessity.\n",
      "Does this sound like you? Apply now to take your first steps toward living the Regeneron Way! We have an inclusive and diverse culture that provides comprehensive benefits including health and wellness programs, fitness centers and equity awards, annual bonuses, and paid time off for eligible employees at all levels!\n",
      "\n",
      "Regeneron is an equal opportunity employer and all qualified applicants will receive consideration for employment without regard to race, color, religion or belief (or lack thereof), sex, nationality, national or ethnic origin, civil status, age, citizenship status, membership of the Traveler community, sexual orientation, disability, genetic information, familial status, marital or registered civil partnership status, pregnancy or parental status, gender identity, gender reassignment, military or veteran status, or any other protected characteristic in accordance with applicable laws and regulations. We will ensure that individuals with disabilities are provided reasonable accommodations to participate in the job application process. Please contact us to discuss any accommodations you think you may need.\n",
      "\n",
      "The salary ranges provided are shown in accordance with U.S. law and apply to U.S. based positions, where the hired candidate will be located in the U.S. If you are outside the U.S, please speak with your recruiter about salaries and benefits in your location. \n",
      "\n",
      "The extraction was completed in: 35.0 minutes and 25.416138172149658 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Scrape the job details.\n",
    "get_job_details(csv_files[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The raw data was  rewritten to existing file and successfully exported as C:\\Users\\temulenbd\\OneDrive\\Desktop\\learn\\github_repo\\cct\\capstone_project\\data_jobads_da.csv.\n"
     ]
    }
   ],
   "source": [
    "# Update the extracted data and save the changes.\n",
    "df_create_export_csv(df_name[2], csv_files[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 20th JAN, 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Setting up for web scraping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define global variables.\n",
    "job_details = []\n",
    "csv_files = ['data_jobads_rn.csv', 'data_jobads_e.csv', 'data_jobads_da.csv']\n",
    "csv_files_20jan = ['data_jobads_rn_20jan.csv', 'data_jobads_e_20jan.csv', 'data_jobads_da_20jan.csv']\n",
    "directory = r'C:\\Users\\temulenbd\\OneDrive\\Desktop\\learn\\github_repo\\cct\\capstone_project'\n",
    "df_name = ['df_rn', 'df_e', 'df_da']\n",
    "df_name_20jan = ['df_rn_20jan', 'df_e_20jan', 'df_da_20jan']\n",
    "first_date = 'January 10, 2024'\n",
    "keywords = ['REGISTERED NURSE', 'ELECTRICIAN', 'DATA ANALYST']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom function to remove duplicate job ads.\n",
    "def remove_duplicates(csv_new, csv_old, df_new, df_old, key_word):\n",
    "    \n",
    "    # Declare global variables.\n",
    "    global directory\n",
    "    \n",
    "    df_new = pd.read_csv(csv_new, index_col=None)\n",
    "    df_old = pd.read_csv(csv_old, index_col=None)\n",
    "    \n",
    "    merged_df = pd.merge(df_new, df_old[['id']], on='id', how='left', indicator=True)\n",
    "\n",
    "    # Filter rows where the job ID is not present in both DataFrames.\n",
    "    df_new = merged_df[merged_df['_merge'] == 'left_only']\n",
    "\n",
    "    # Drop the indicator column.\n",
    "    df_new = df_new.drop('_merge', axis=1)\n",
    "    \n",
    "    # Create the file path for CSV export.\n",
    "    file_path = os.path.join(directory, csv_new)\n",
    "    \n",
    "    # Update the existing DataFrame.\n",
    "    df_new.to_csv(file_path, index=False)\n",
    "    \n",
    "    print(f'The raw data was updated successfully as {file_path}.')\n",
    "    print(f'There are {df_new.shape[0]} new job ads added since {first_date} with the keyword <{key_word}>.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom function to extract job details from the available hyperlinks.\n",
    "def get_job_details(csv):\n",
    "    \n",
    "    # Declare global variables.\n",
    "    global job_details\n",
    "    \n",
    "    # Set up Chrome webdriver options.\n",
    "    option=Options()\n",
    "    option.add_experimental_option('debuggerAddress', 'localhost:0820')\n",
    "    \n",
    "    # Specify the start time.\n",
    "    start = time.time()\n",
    "    \n",
    "    # Initialize Chrome driver.\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=option)\n",
    "    \n",
    "    # Access each hyperlink, retrieve information about the job, and store it in the global variable 'job_details'.\n",
    "    df_ads = pd.read_csv(csv, index_col=None)\n",
    "    total_rows = df_ads.shape[0]\n",
    "    \n",
    "    for x in range(0, total_rows):\n",
    "        link = df_ads.iat[x, 2]\n",
    "        driver.get(link)\n",
    "        sleep(randint(2, 4))\n",
    "    \n",
    "        job_page = driver.find_element(By.ID, 'jobDescriptionText')\n",
    "        job_details.append(job_page.text)\n",
    "        sleep(randint(2, 4))\n",
    "        \n",
    "    # Specify the end time.\n",
    "    end = time.time()\n",
    "    \n",
    "    # Check results.\n",
    "    print(f'Total number of extracted job ads details: {len(job_details)}.\\n')\n",
    "    print('EXAMPLE:')\n",
    "    print(job_details[randint(0, total_rows)], '\\n')\n",
    "    print(f'The extraction was completed in: {(end - start) // 60} minutes and {(end - start) % 60} seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a costum function to rewrite extracted information into existing csv files.\n",
    "def df_create_export_csv(new_df, csv):\n",
    "\n",
    "    # Declare global variables.\n",
    "    global job_details\n",
    "    global directory\n",
    "    \n",
    "    # Create a new pandas Dataframe using the ads csv file.\n",
    "    new_df = pd.read_csv(csv, index_col=None)\n",
    "    new_df['job_description'] = job_details\n",
    "    \n",
    "    # Create the file path for CSV export.\n",
    "    file_path = os.path.join(directory, csv)\n",
    "    \n",
    "    # Export the DataFrame to CSV file.\n",
    "    new_df.to_csv(file_path, index=False)\n",
    "    \n",
    "    print(f\"The raw data was rewritten to existing file and successfully exported as {file_path}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Remove duplicates and web scraping.\n",
    "**registered nurse ads**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The raw data was updated successfully as C:\\Users\\temulenbd\\OneDrive\\Desktop\\learn\\github_repo\\cct\\capstone_project\\data_jobads_rn_20jan.csv.\n",
      "There are 194 new job ads added since January 10, 2024 with the keyword <REGISTERED NURSE>.\n"
     ]
    }
   ],
   "source": [
    "remove_duplicates(csv_files_20jan[0], csv_files[0], df_name_20jan[0], df_name[0], keywords[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of extracted job ads details: 194.\n",
      "\n",
      "EXAMPLE:\n",
      "Description:\n",
      "Cpl Healthcare are seeking a Staff Nurse to join an excellent Ophthalmology Clinic\n",
      "Our client is seeking a Staff Nurse to join their growing team. This clinic specializes in eye surgery and procedures on an outpatient basis. Prior ophthalmology experience not required as training will be provided\n",
      "Shift Pattern: 4x10hour shifts per week\n",
      "Applicant Requirements\n",
      "NMBI Registered General Nurse\n",
      "Previous experience in an acute surgical environment desirable\n",
      "Good teamwork skills\n",
      "Willingness to learn\n",
      "Excellent clinical skills\n",
      "Excellent communication skills\n",
      "\n",
      "EMAIL: louise.omeara@cplhealthcare.com\n",
      "Ref.no.:\n",
      "JO-2307-519025\n",
      "Locations:\n",
      "Dublin\n",
      "Salary:\n",
      "€33000 - €50000\n",
      "Employment type:\n",
      "Full Time;\n",
      "Tags:\n",
      "Clinic Nurse,Day Nurse,ENT,Eye,Laser Surgery,Nurse,Nursing,Ophthalmology,Surgical\n",
      "\n",
      "EMAIL: louise.omeara@cplhealthcare.com\n",
      "Ref.no.:\n",
      "JO-2307-519025\n",
      "Locations:\n",
      "Dublin\n",
      "Salary:\n",
      "€33000 - €50000\n",
      "Employment type:\n",
      "Full Time;\n",
      "Tags:\n",
      "Clinic Nurse,Day Nurse,ENT,Eye,Laser Surgery,Nurse,Nursing,Ophthalmology,Surgical \n",
      "\n",
      "The extraction was completed in: 21.0 minutes and 15.305526971817017 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Scrape the job details.\n",
    "get_job_details(csv_files_20jan[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The raw data was rewritten to existing file and successfully exported as C:\\Users\\temulenbd\\OneDrive\\Desktop\\learn\\github_repo\\cct\\capstone_project\\data_jobads_rn_20jan.csv.\n"
     ]
    }
   ],
   "source": [
    "# Update the extracted data and save the changes.\n",
    "df_create_export_csv(df_name[0], csv_files_20jan[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset variable.\n",
    "job_details = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**electrician ads**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The raw data was updated successfully as C:\\Users\\temulenbd\\OneDrive\\Desktop\\learn\\github_repo\\cct\\capstone_project\\data_jobads_e_20jan.csv.\n",
      "There are 54 new job ads added since January 10, 2024 with the keyword <ELECTRICIAN>.\n"
     ]
    }
   ],
   "source": [
    "remove_duplicates(csv_files_20jan[1], csv_files[1], df_name_20jan[1], df_name[1], keywords[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of extracted job ads details: 54.\n",
      "\n",
      "EXAMPLE:\n",
      "Maintenance Electrician required Dublin, Salary 50k – 55k+ Bonus.\n",
      "Your new Company\n",
      "This company is part of one of the UK & Irelands largest water companies who provides water and recycling services to over 6 million customers in England. Operating for over 20 years in the Irish market they currently operate one of Europe’s largest wastewater treatment plants, at which they currently treat over 50% of Ireland’s wastewater. The are a proven leader in the provision of innovative water, wastewater, and resource recycling solutions for a range of sectors which include municipal, industrial, and commercial industries in Ireland.\n",
      "From design and engineering to construction, through to site operation and management, they have a proven track record in the provision of Lean water and wastewater solutions that increase efficiency, reduce carbon footprint, and minimize operational cost for our customers.\n",
      "Your new role\n",
      "This role enquires you to provide expertise on all electrical systems, and problem solving/fault finding of electrical systems & equipment throughout the site to secure the continuous operation of site. Some of your main duties will be:\n",
      "· Troubleshoot and repair electrical components, panels, and systems.\n",
      "· Install and maintain industrial communication systems, heavy duty electrical equipment.\n",
      "· Perform quality testing and inspections.\n",
      "· Wire up, gland, terminate pumps, motors, VSDs, circuit boards and panels, install new conduits, trays, ladder and pull new cables where necessary.\n",
      "· Recording of all activities on CMMS\n",
      "· Provide out of hours support to plant operations during plant start-up, running and shutdown activities.\n",
      "· Participate in on-call Rota.\n",
      "· Follow all health and safety rules and regulations. Keep work area clean, safe, and orderly.\n",
      "· Perform other duties as deemed necessary by the Maintenance Engineer.\n",
      "· The position of Maintenance Electrician requires flexibility and availability as demanded by the needs of a continuous plant and process. You will be part of an on-call system providing electrical cover and as a result, after-hours and weekend interaction with the plant will be a routine aspect of the job.\n",
      "· Awareness of national and industrial technical standards and regulations\n",
      "· Be a good team player and be analytically minded in linking financial performance to operational efficiency.\n",
      "· Be able to work in a very fast paced environment where priorities may change regularly.\n",
      "· Willing to perform physically demanding tasks such as climbing ladders, working at heights and/or in confined spaces.\n",
      "· Problem solving and fault finding with MV/LV AC and LC DC switchgear and distribution systems.\n",
      "· Good command of electrical drawings\n",
      "· A disciplined step-by-step systematic approach to problems\n",
      "· Fault finding under time pressure.\n",
      "What you will need to succeed\n",
      "· 5 years relevant electrical experience in a heavy industrial environment\n",
      "· Electrical National Craft Certificate essential.\n",
      "· Very strong fault diagnosis & solving experience.\n",
      "· Ability to read complex electrical drawings and P&IDs\n",
      "· Specific experience with Rockwell / Allen Bradley Control Systems and PLCs, Endress and Hauser and ABB instrumentation, Auma actuators is advantageous.\n",
      "· Knowledge and experience of ATEX zoned areas is also advantageous.\n",
      "What you will get in return\n",
      "You will get A very competitive salary. With a range of benefits some of these benefits include:\n",
      "· A salary range of 50k – 55k\n",
      "· Overtime\n",
      "· Performance related bonus of up to 5% of salary\n",
      "· on call rate of €20 per day whilst on call.\n",
      "What you need to do now\n",
      "If you're interested in this role, click 'apply now' to forward an up-to-date copy of your CV, or call us now.\n",
      "If this job isn't quite right for you but you are looking for a new position, please contact us for a confidential discussion on your career.\n",
      "Job Type: Full-time\n",
      "Salary: €50,000.00-€55,000.00 per year\n",
      "Schedule:\n",
      "8 hour shift\n",
      "Day shift\n",
      "Monday to Friday\n",
      "Overtime\n",
      "Supplemental pay types:\n",
      "Bonus pay\n",
      "Overtime pay\n",
      "Performance bonus\n",
      "Ability to commute/relocate:\n",
      "Dublin, CO. Dublin: reliably commute or plan to relocate before starting work (preferred)\n",
      "Work Location: In person \n",
      "\n",
      "The extraction was completed in: 5.0 minutes and 46.99110245704651 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Scrape the job details.\n",
    "get_job_details(csv_files_20jan[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The raw data was rewritten to existing file and successfully exported as C:\\Users\\temulenbd\\OneDrive\\Desktop\\learn\\github_repo\\cct\\capstone_project\\data_jobads_e_20jan.csv.\n"
     ]
    }
   ],
   "source": [
    "# Update the extracted data and save the changes.\n",
    "df_create_export_csv(df_name[1], csv_files_20jan[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset variable.\n",
    "job_details = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**data analyst ads**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The raw data was updated successfully as C:\\Users\\temulenbd\\OneDrive\\Desktop\\learn\\github_repo\\cct\\capstone_project\\data_jobads_da_20jan.csv.\n",
      "There are 85 new job ads added since January 10, 2024 with the keyword <DATA ANALYST>.\n"
     ]
    }
   ],
   "source": [
    "remove_duplicates(csv_files_20jan[2], csv_files[2], df_name_20jan[2], df_name[2], keywords[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of extracted job ads details: 85.\n",
      "\n",
      "EXAMPLE:\n",
      "Job Title: Strategy Analyst- Fintech\n",
      "Sector: Fintech\n",
      "Location: Dublin/Hybrid\n",
      "Salary: DOE plus benefits\n",
      "\n",
      "Our Client\n",
      "\n",
      "Our client is an award-winning Fintech company headquartered in Dublin. Due to huge growth, there is a newly created Analyst opportunity within the Strategy function.\n",
      "\n",
      "Why should you apply?\n",
      "\n",
      "This is an extremely varied role working with the strategy Director on key company growth projects. This role will put you at the centre of decision making in a team responsible for driving the growth of new products in new markets (including US, Europe, and Asia). There is real scope for professional growth here, visibility of your achievements on the company’s success, and the chance to work in a collaborative and open environment.\n",
      "\n",
      "Who should apply?\n",
      "\n",
      "You will be a data-driven individual with at least 3 years’ experience within Consulting, Strategy, or Transformation, as well as:\n",
      "Professional Services, Financial Services or Tech experience beneficial\n",
      "SaaS experience beneficial\n",
      "Excellent data analysis skills\n",
      "The desire to work in Fintech\n",
      "Strong commercial acumen and problem-solving skills\n",
      "Demonstrable presentation, influencing and communication skills\n",
      "ACA/CFA qualification beneficial but not essential\n",
      "Role and Reporting Lines\n",
      "\n",
      "Reporting to the Strategy Director you will work across new market analysis, market positioning, GTM and transformation activities, more specifically:\n",
      "Analyse and create business cases for investment, contributing to revenue growth\n",
      "Develop operating models and internal transformation initiatives\n",
      "Partner with clients on strategy\n",
      "Competitor analysis\n",
      "Interested in this position?\n",
      "To apply, please submit your CV to Mark Baker who is managing this assignment via the link below or to inquire further please contact us directly on 01 529 4200. \n",
      "\n",
      "The extraction was completed in: 9.0 minutes and 7.9710893630981445 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Scrape the job details.\n",
    "get_job_details(csv_files_20jan[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The raw data was rewritten to existing file and successfully exported as C:\\Users\\temulenbd\\OneDrive\\Desktop\\learn\\github_repo\\cct\\capstone_project\\data_jobads_da_20jan.csv.\n"
     ]
    }
   ],
   "source": [
    "# Update the extracted data and save the changes.\n",
    "df_create_export_csv(df_name[2], csv_files_20jan[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I.III Job offers' data processing (cleaning and transforming)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Setting up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define global variables.\n",
    "label_value = ['registered_nurse', 'electrician', 'data_analyst', 'registered_nurse', 'electrician', 'data_analyst']\n",
    "csv_file_name = ['data_jobads_rn.csv', 'data_jobads_e.csv', 'data_jobads_da.csv', 'data_jobads_rn_20jan.csv', 'data_jobads_e_20jan.csv', 'data_jobads_da_20jan.csv']\n",
    "data_frame = ['df1', 'df2', 'df3', 'df4', 'df5', 'df6']\n",
    "to_remove = ['salary', 'schedule', 'benefit', 'location', 'job type', 'office', 'tag', 'employment type', 'email', 'ref.no', \n",
    "             'contact name', 'job ref', 'offer in return', 'job title', 'received by', 'signature date', '______', 'block capitals']\n",
    "date_of_download = ['January 10, 2024', 'January 20, 2024']\n",
    "before_30_days = ['before December 11, 2023', 'before December 21, 2023']\n",
    "directory = r'C:\\Users\\temulenbd\\OneDrive\\Desktop\\learn\\github_repo\\cct\\capstone_project'\n",
    "df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom function to import a CSV file, add a label column, and return the DataFrame.\n",
    "def import_and_label(csv, value):\n",
    "    \n",
    "    # Declare global variables.\n",
    "    global df\n",
    "    \n",
    "    # Read the CSV file into a DataFrame.\n",
    "    new_df = pd.read_csv(csv, index_col=None)\n",
    "    \n",
    "    # Add a new column 'label' with the specified value.\n",
    "    new_df['label'] = value\n",
    "    \n",
    "    # Concatenate the existing DataFrame 'df' with the new DataFrame.\n",
    "    df = pd.concat([df, new_df], ignore_index=True)\n",
    "    \n",
    "    print('The values of the <'+ csv + '> file were successfully added to the <df>.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom function to replace 'Just posted' or 'Today' with 'January 10, 2024'.\n",
    "def replace_just_posted_10th(date):\n",
    "    global date_of_download\n",
    "    date_download = date_of_download[0]\n",
    "    return date_download if 'Just posted' in str(date) or 'Today' in str(date) else date\n",
    "\n",
    "# Define a custom function to replace 'Just posted' or 'Today' with 'January 20, 2024'.\n",
    "def replace_just_posted_20th(date):\n",
    "    global date_of_download\n",
    "    date_download = date_of_download[1]\n",
    "    return date_download if 'Just posted' in str(date) or 'Today' in str(date) else date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom function to extract and replace values with relevant information.\n",
    "def remove_replace_elements_10th(date):\n",
    "    global date_of_download\n",
    "    if date and ('day' in str(date) or 'days' in str(date)):\n",
    "        days_ago = int(re.search(r'(\\d+) (?:day|days) ago', str(date)).group(1))\n",
    "        new_date = datetime.strptime(date_of_download[0], '%B %d, %Y') - timedelta(days=days_ago)\n",
    "        return new_date.strftime('%B %d, %Y')\n",
    "    else:\n",
    "        return date\n",
    "    \n",
    "# Define a custom function extract and replace with relevant information.    \n",
    "def remove_replace_elements_20th(date):\n",
    "    global date_of_download\n",
    "    if date and ('day' in str(date) or 'days' in str(date)):\n",
    "        days_ago = int(re.search(r'(\\d+) (?:day|days) ago', str(date)).group(1))\n",
    "        new_date = datetime.strptime(date_of_download[1], '%B %d, %Y') - timedelta(days=days_ago)\n",
    "        return new_date.strftime('%B %d, %Y')\n",
    "    else:\n",
    "        return date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the costum function to find and remove unnecessary or private information.\n",
    "def remove_elements_with_colon(column_value, remove_value):\n",
    "        \n",
    "    # Convert the value to lowercase.\n",
    "    column_value = column_value.lower()\n",
    "    \n",
    "    # Split each value of the column into a list using '\\n' as a separator.\n",
    "    elements = column_value.split('\\n')\n",
    "    \n",
    "    # Find the index of the element containing given value.\n",
    "    index_of_element = next((i for i, elements in enumerate(elements) if remove_value in elements and ':' in elements), None)\n",
    "    \n",
    "    # Check if the first conditions are present in the text.\n",
    "    if index_of_element is not None:\n",
    "        next_colon_index = next((j for j in range(index_of_element + 1, len(elements)) if ':' in elements[j]), None)\n",
    "        \n",
    "        # Add an extra condition to check the second conditions are present in the text.\n",
    "        if next_colon_index is not None:\n",
    "            del elements[index_of_element:next_colon_index]\n",
    "            return '\\n'.join(elements)\n",
    "        else:\n",
    "            return column_value\n",
    "        \n",
    "    else:\n",
    "        return column_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the costum function to find and remove unnecessary or private information.\n",
    "def remove_unnec_lines(column_value, remove_value):\n",
    "    # Split each value of the column into a list using '\\n' as a separator.\n",
    "    elements = column_value.split('\\n')\n",
    "\n",
    "    # Function to check if a line contains an email address, phone number, or link\n",
    "    def is_unwanted_line(line):\n",
    "        email_pattern = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b'\n",
    "        phone_pattern = r'\\b\\d+[-.\\s+]?\\d+[-.\\s+]?\\d+\\b'\n",
    "        link_pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    "\n",
    "        return re.search(email_pattern, line) or re.search(phone_pattern, line) or re.search(link_pattern, line)\n",
    "\n",
    "    # Remove lines containing the specified remove_value\n",
    "    index_of_element = next((i for i, element in enumerate(elements) if remove_value in element), None)\n",
    "    if index_of_element is not None:\n",
    "        elements.pop(index_of_element)\n",
    "\n",
    "    # Remove lines containing emails, phone numbers, and links\n",
    "    elements = [element for element in elements if not is_unwanted_line(element)]\n",
    "\n",
    "    return '\\n'.join(elements)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. CLEANING AND TRANSFORMATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**january 10, 2024**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The values of the <data_jobads_rn.csv> file were successfully added to the <df>.\n",
      "The values of the <data_jobads_e.csv> file were successfully added to the <df>.\n",
      "The values of the <data_jobads_da.csv> file were successfully added to the <df>.\n",
      "--------------------------------\n",
      "The newly created <df> has 1034 rows and 6 columns.\n"
     ]
    }
   ],
   "source": [
    "# Merge csv file from 10th of January into one DataFrame.\n",
    "for x in range(3):\n",
    "    import_and_label(csv_file_name[x], label_value[x])\n",
    "\n",
    "print('--------------------------------')\n",
    "rows = df.shape[0]\n",
    "columns = df.shape[1]\n",
    "print(f'The newly created <df> has {rows} rows and {columns} columns.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 114 duplicate rows were removed from the <df>\n",
      "Now, the <df> contains 920 rows and 6 columns.\n"
     ]
    }
   ],
   "source": [
    "# Remove duplicate rows based on the 'id' column and keep the original rows.\n",
    "df = df.drop_duplicates(subset='id', keep='first')\n",
    "\n",
    "print(f'There are {rows-df.shape[0]} duplicate rows were removed from the <df>')\n",
    "print(f'Now, the <df> contains {df.shape[0]} rows and {df.shape[1]} columns.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before December 11, 2023    425\n",
      "unknown                      72\n",
      "January 08, 2024             71\n",
      "December 20, 2023            44\n",
      "January 05, 2024             40\n",
      "December 22, 2023            37\n",
      "January 10, 2024             30\n",
      "January 09, 2024             28\n",
      "January 03, 2024             28\n",
      "January 04, 2024             21\n",
      "January 06, 2024             18\n",
      "December 13, 2023            14\n",
      "December 19, 2023            12\n",
      "December 14, 2023            10\n",
      "December 21, 2023            10\n",
      "December 23, 2023             8\n",
      "December 12, 2023             8\n",
      "December 16, 2023             7\n",
      "December 30, 2023             6\n",
      "January 02, 2024              6\n",
      "December 15, 2023             5\n",
      "January 07, 2024              5\n",
      "December 29, 2023             3\n",
      "December 18, 2023             2\n",
      "January 01, 2024              2\n",
      "December 28, 2023             2\n",
      "December 26, 2023             2\n",
      "December 31, 2023             1\n",
      "December 11, 2023             1\n",
      "December 24, 2023             1\n",
      "December 27, 2023             1\n",
      "Name: date, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Extract and replace values of the 'date' column.\n",
    "df['date'] = df['date'].replace(['not available', 'Hiring ongoing', None], 'unknown').replace(['Posted 30+ days ago'], before_30_days[0]).apply(replace_just_posted_10th).apply(remove_replace_elements_10th)\n",
    "\n",
    "# Save the values and reset the <df>.\n",
    "df_10th = df.copy()\n",
    "df = pd.DataFrame()\n",
    "\n",
    "# Assuming df is your DataFrame and 'date' is the column\n",
    "date_counts = df_10th['date'].value_counts()\n",
    "\n",
    "# Print the counts for each unique value in the 'date' column\n",
    "print(date_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**january 20, 2024**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The values of the <data_jobads_rn_20jan.csv> file were successfully added to the <df>.\n",
      "The values of the <data_jobads_e_20jan.csv> file were successfully added to the <df>.\n",
      "The values of the <data_jobads_da_20jan.csv> file were successfully added to the <df>.\n",
      "--------------------------------\n",
      "The newly created <df> has 333 rows and 6 columns.\n"
     ]
    }
   ],
   "source": [
    "# Merge all csv file into one DataFrame\n",
    "for x in range(3, 6):\n",
    "    import_and_label(csv_file_name[x], label_value[x])\n",
    "\n",
    "print('--------------------------------')\n",
    "rows = df.shape[0]\n",
    "columns = df.shape[1]\n",
    "print(f'The newly created <df> has {rows} rows and {columns} columns.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 14 duplicate rows were removed from the <df>\n",
      "Now, the <df> contains 319 rows and 6 columns.\n"
     ]
    }
   ],
   "source": [
    "# Remove duplicate rows based on the 'id' column and keep the original rows.\n",
    "df = df.drop_duplicates(subset='id', keep='first')\n",
    "\n",
    "print(f'There are {rows-df.shape[0]} duplicate rows were removed from the <df>')\n",
    "print(f'Now, the <df> contains {df.shape[0]} rows and {df.shape[1]} columns.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "January 18, 2024            61\n",
      "unknown                     40\n",
      "before December 21, 2023    35\n",
      "January 20, 2024            32\n",
      "January 17, 2024            30\n",
      "January 19, 2024            29\n",
      "January 16, 2024            26\n",
      "January 12, 2024            19\n",
      "January 13, 2024            14\n",
      "January 11, 2024             9\n",
      "January 10, 2024             8\n",
      "January 15, 2024             7\n",
      "January 09, 2024             3\n",
      "January 14, 2024             1\n",
      "December 22, 2023            1\n",
      "January 06, 2024             1\n",
      "January 02, 2024             1\n",
      "January 03, 2024             1\n",
      "December 23, 2023            1\n",
      "Name: date, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Extract and replace values of the 'date' column.\n",
    "df['date'] = df['date'].replace(['not available', 'Hiring ongoing', None], 'unknown').replace(['Posted 30+ days ago'], before_30_days[1]).apply(replace_just_posted_20th).apply(remove_replace_elements_20th)\n",
    "\n",
    "# Save the values and reset the <df>.\n",
    "df_20th = df.copy()\n",
    "df = pd.DataFrame()\n",
    "\n",
    "# Assuming df is your DataFrame and 'date' is the column\n",
    "date_counts = df_20th['date'].value_counts()\n",
    "\n",
    "# Print the counts for each unique value in the 'date' column\n",
    "print(date_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**combining and finalizing the process in merged data frame**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The newly created <df> has 1239 rows and 6 columns.\n"
     ]
    }
   ],
   "source": [
    "# Merge two dataframes into one.\n",
    "df = pd.concat([df_10th, df_20th], ignore_index=True)\n",
    "rows = df.shape[0]\n",
    "columns = df.shape[1]\n",
    "\n",
    "print(f'The newly created <df> has {rows} rows and {columns} columns.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 0 duplicate rows were removed from the <df>\n",
      "Now, the <df> contains 1239 rows and 6 columns.\n"
     ]
    }
   ],
   "source": [
    "# Remove duplicate rows based on the 'id' column and keep the original rows.\n",
    "df = df.drop_duplicates(subset='link', keep='first')\n",
    "\n",
    "print(f'There are {rows-df.shape[0]} duplicate rows were removed from the <df>')\n",
    "print(f'Now, the <df> contains {df.shape[0]} rows and {df.shape[1]} columns.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RANDOM EXAMPLES:\n",
      "\n",
      "senior analyst – trade spend management\n",
      "\n",
      "glanbia\n",
      "\n",
      "join this dynamic team focused on delivering better nutrition for every step of life’s journey\n",
      "\n",
      "the opportunity\n",
      "\n",
      "the senior analyst – trade spend management will be responsible for leading the development of a trade spend deals glossary and associated processes for tracking, accounting for and management of trade spend and deductions relating to gpn europe’s sports nutrition brands. with initial focus on the uk region, and subsequently expanded to other european markets.\n",
      "\n",
      "this position will be part of gpn’s europe finance team, reporting into the finance director for uk & ireland.\n",
      "proactively absorb learnings from the trade spend deals glossary and management processes developed within the us business, and use this to map phases of development for the uk (phase 1), european and international businesses\n",
      "operate as the finance owner of uk and ireland trade spend process and deductions for sports nutrition and lifestyle brands\n",
      "implement full tracking and maintenance of accurate and up-to-date sub ledgers of actualised trade spend to ensure trade spend is accurately recorded and analysed. to include building of gl sub-account recording process beneath edlp / working / non-working trade spend, aligned to us methodology\n",
      "ensure all deductions (promotional, fixed & customer rebates) are valid and appropriate based on performance and auditing\n",
      "develop trade spend reporting and consistent communication to relevant parties, such as customer sales teams, gpn’s finance group & gpn leadership\n",
      "ensure accurate deduction clearing through working closely with internal sales teams and glanbia shared services. administer and enforce deduction management policies and ensure deductions are appropriately coded to the correct customers, brands, skus, etc.\n",
      "process journal entries and assist with trade spend accruals and reconciliation processes\n",
      "work closely with other rgm team members, finance teams and sales teams, to assess and provide insight on trade effectiveness & efficiency\n",
      "support regional finance directors and sales directors in highlighting risks and opportunities with trade spend on a rolling basis\n",
      "consistently explore better processes and ways of tracking, accounting for, and reporting on trade spend. support the business in helping to define the key components of a system-based trade spend management system as part of the gpn it roadmap\n",
      "\n",
      "the skills you will bring to the team\n",
      "degree in accounting, finance, business (or similar) required\n",
      "a self-starter with an inquisitive mindset, ability to work independenetly, and a desire to drive process improvements\n",
      "highly analytical mind-set with experience in data analysis and manipulating large quantities of information/ data\n",
      "excellent microsoft excel skills\n",
      "ability to work in a team environment, and influence and partner effectively with key internal stakeholders across finance and other finctions (e.g. sales and customer services)\n",
      "trade spend management system experience beneficial\n",
      "experience of working with sap and business objects (reporting tool) strongly preferred\n",
      "accurate and detail orientated in approach to work and perform tasks\n",
      "\n",
      "if you think you have what it takes, but don't necessarily meet every single point on the job description, please apply!\n",
      "\n",
      "where and how you will work\n",
      "the opportunity will be based in dublin with hybrid working arrangements available through our smart working model which allows you a greater choice in how you work and live, giving you a better work-life balance.\n",
      "\n",
      "what we would like to offer you!\n",
      "\n",
      "about glanbia\n",
      "\n",
      "at glanbia, we celebrate diversity, because we know that our individual strengths make us stronger together. we welcome and encourage interest from a variety of candidates, we will give your application consideration, without regard to race, color, religion, sex, sexual orientation, gender perception or identity, national origin, age, marital status, protected veteran status, or disability status.\n",
      "\n",
      "at glanbia, our culture celebrates individuality, knowing that together we are more.\n",
      "\n",
      "at glanbia, we celebrate diversity, because we know that our individual strengths make us stronger together. we welcome and encourage interest from a wide variety of candidates, and we will give your application consideration, without regard to race, color, religion, sex, sexual orientation, gender perception or identity, national origin, age, marital status, or disability status.\n",
      "\n",
      "at glanbia our culture will celebrate individuality, knowing that together we are more.\n",
      "---------\n",
      "clinical nurse manager 2\n",
      "\n",
      "prosper fingal has been a leader in providing high-quality, person-centred, community-based services to adults with intellectual disabilities and autism for over 40 years. join our wonderful team and help adults with intellectual disabilities in north county dublin reach their full potential and connect with their communities.\n",
      "\n",
      "hours: 37 hours per week\n",
      "contract: permanent\n",
      "\n",
      "we are looking for an experienced and qualified clinical nurse manager (cnm 2) to join our multidisciplinary team to support the clinical provision within prosper. the main responsibilities of this post are but are not limited to:\n",
      "\n",
      "provide clinical advice and supports to frontline social care teams that do not have a nursing background.\n",
      "assess, plan, implement and evaluate nursing and clinical care\n",
      "oversee and review best practice in the safe management and administration of medication throughout services in line with company policy\n",
      "conduct medication audits in day and respite care\n",
      "monitor reports of medication incidents / errors across the company, including logging reports and identifying any follow up actions. report on trends in medication incidents / errors at various levels\n",
      "act as a mentor to the nursing staff team\n",
      "participate on the interview panel for recruitment needs of the nursing team\n",
      "support the development of health, intimate care, medication plans for all service users as required\n",
      "facilitate and oversee vaccinations for staff and service users as required\n",
      "coordinate in-house health clinics if required\n",
      "attend medical / health appointments with service users, if required\n",
      "liaise with external clinics if required e.g. epilepsy, diabetes\n",
      "provide health related guidance, advise and information to services including individuals supported by prosper, their relatives and staff\n",
      "manage the use of health-related supplies, equipment and health devices across services\n",
      "liaise with other health care professionals and build community links with primary care teams, medical trainers etc\n",
      "respond to clinical and health needs across services as required\n",
      "be familiar with company policies and protocols and support in the development of health-related policy, guidelines as required\n",
      "facilitate health related training as required\n",
      "\n",
      "the successful applicant must have:\n",
      "third-level nursing qualification and be registered with the nmbi is essential\n",
      "a minimum of three years' experience as a cnm1\n",
      "strong clinical, managerial and administrative and computer skills\n",
      "excellent knowledge and understanding of relevant health act regulations, hiqa standards and all other relevant legislation\n",
      "a thorough understanding of, and experience with, the provision of service and supports to adults with intellectual disabilities as outlined in current national disability policy and underlying regulatory requirements\n",
      "excellent knowledge of all aspects of medication management\n",
      "a full clean manual driving licence\n",
      "excellent communication skills\n",
      "the successful applicant will lead by example in demonstrating our core values of being professional, respectful, positive, progressive and ethical at all times.\n",
      "\n",
      "what we offer:\n",
      "a rewarding working environment\n",
      "competitive rates of pay\n",
      "generous annual leave\n",
      "sick pay scheme\n",
      "pension\n",
      "maternity leave allowance * (subject to service length)\n",
      "career progression opportunities\n",
      "training / continuing professional development\n",
      "excellent facilities\n",
      "work-life balance\n",
      "employee assistance program (eap) which offers free advice and counselling services, for employees and their immediate families\n",
      "cycle to work scheme\n",
      "refer a friend bonus\n",
      "regular updating of mandatory training along with non-mandatory, role specific training if required\n",
      "\n",
      "should you have the outlined qualifications and experience and feel you align with our organisation that upholds the highest of standards for the people we support, we would like to hear from you.\n",
      "\n",
      "prosper is an equal opportunities employer\n"
     ]
    }
   ],
   "source": [
    "# Remove any private and unnecessary information from the job details.\n",
    "for val_rem in to_remove:\n",
    "    df['job_description'] = df['job_description'].apply(lambda x: remove_elements_with_colon(x, val_rem)\n",
    "                                                        ).apply(lambda x:remove_unnec_lines(x, val_rem))\n",
    "\n",
    "print('RANDOM EXAMPLES:\\n')\n",
    "print(df.iat[randint(0, 1034), 4])\n",
    "print('---------')\n",
    "print(df.iat[randint(0, 1034), 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all unnecessary symbols.\n",
    "df['job_description'] = df['job_description'].str.replace(r'[^:;,.\\s\\w]', '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Create the file path for CSV export.\n",
    "file_path = os.path.join(directory, 'data_jobads_final.csv')\n",
    "    \n",
    "# Export the DataFrame to CSV file.\n",
    "df.to_csv(file_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
